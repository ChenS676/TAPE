Number of available CUDA devices: 1
Number of available CUDA devices: 1
not loaded 0 papers.
not loaded 0 paperid.
create graph: 0.07454395294189453
[2485]
original num of nodes: 2485
num of nodes after lcc: 2485
num of edges after lcc: 10418
num of texts in dataset: 2485
train adj shape: 8336
train: original length 4168
train: downsampled length 4168
valid: original length 781
valid: downsampled length 781
test: original length 260
test: downsampled length 260
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.token_type_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias
encoder.layer.0.attention.self.query.weight
encoder.layer.0.attention.self.query.bias
encoder.layer.0.attention.self.key.weight
encoder.layer.0.attention.self.key.bias
encoder.layer.0.attention.self.value.weight
encoder.layer.0.attention.self.value.bias
encoder.layer.0.attention.output.dense.weight
encoder.layer.0.attention.output.dense.bias
encoder.layer.0.attention.output.LayerNorm.weight
encoder.layer.0.attention.output.LayerNorm.bias
encoder.layer.0.intermediate.dense.weight
encoder.layer.0.intermediate.dense.bias
encoder.layer.0.output.dense.weight
encoder.layer.0.output.dense.bias
encoder.layer.0.output.LayerNorm.weight
encoder.layer.0.output.LayerNorm.bias
encoder.layer.1.attention.self.query.weight
encoder.layer.1.attention.self.query.bias
encoder.layer.1.attention.self.key.weight
encoder.layer.1.attention.self.key.bias
encoder.layer.1.attention.self.value.weight
encoder.layer.1.attention.self.value.bias
encoder.layer.1.attention.output.dense.weight
encoder.layer.1.attention.output.dense.bias
encoder.layer.1.attention.output.LayerNorm.weight
encoder.layer.1.attention.output.LayerNorm.bias
encoder.layer.1.intermediate.dense.weight
encoder.layer.1.intermediate.dense.bias
encoder.layer.1.output.dense.weight
encoder.layer.1.output.dense.bias
encoder.layer.1.output.LayerNorm.weight
encoder.layer.1.output.LayerNorm.bias
encoder.layer.2.attention.self.query.weight
encoder.layer.2.attention.self.query.bias
encoder.layer.2.attention.self.key.weight
encoder.layer.2.attention.self.key.bias
encoder.layer.2.attention.self.value.weight
encoder.layer.2.attention.self.value.bias
encoder.layer.2.attention.output.dense.weight
encoder.layer.2.attention.output.dense.bias
encoder.layer.2.attention.output.LayerNorm.weight
encoder.layer.2.attention.output.LayerNorm.bias
encoder.layer.2.intermediate.dense.weight
encoder.layer.2.intermediate.dense.bias
encoder.layer.2.output.dense.weight
encoder.layer.2.output.dense.bias
encoder.layer.2.output.LayerNorm.weight
encoder.layer.2.output.LayerNorm.bias
encoder.layer.3.attention.self.query.weight
encoder.layer.3.attention.self.query.bias
encoder.layer.3.attention.self.key.weight
encoder.layer.3.attention.self.key.bias
encoder.layer.3.attention.self.value.weight
encoder.layer.3.attention.self.value.bias
encoder.layer.3.attention.output.dense.weight
encoder.layer.3.attention.output.dense.bias
encoder.layer.3.attention.output.LayerNorm.weight
encoder.layer.3.attention.output.LayerNorm.bias
encoder.layer.3.intermediate.dense.weight
encoder.layer.3.intermediate.dense.bias
encoder.layer.3.output.dense.weight
encoder.layer.3.output.dense.bias
encoder.layer.3.output.LayerNorm.weight
encoder.layer.3.output.LayerNorm.bias
encoder.layer.4.attention.self.query.weight
encoder.layer.4.attention.self.query.bias
encoder.layer.4.attention.self.key.weight
encoder.layer.4.attention.self.key.bias
encoder.layer.4.attention.self.value.weight
encoder.layer.4.attention.self.value.bias
encoder.layer.4.attention.output.dense.weight
encoder.layer.4.attention.output.dense.bias
encoder.layer.4.attention.output.LayerNorm.weight
encoder.layer.4.attention.output.LayerNorm.bias
encoder.layer.4.intermediate.dense.weight
encoder.layer.4.intermediate.dense.bias
encoder.layer.4.output.dense.weight
encoder.layer.4.output.dense.bias
encoder.layer.4.output.LayerNorm.weight
encoder.layer.4.output.LayerNorm.bias
encoder.layer.5.attention.self.query.weight
encoder.layer.5.attention.self.query.bias
encoder.layer.5.attention.self.key.weight
encoder.layer.5.attention.self.key.bias
encoder.layer.5.attention.self.value.weight
encoder.layer.5.attention.self.value.bias
encoder.layer.5.attention.output.dense.weight
encoder.layer.5.attention.output.dense.bias
encoder.layer.5.attention.output.LayerNorm.weight
encoder.layer.5.attention.output.LayerNorm.bias
encoder.layer.5.intermediate.dense.weight
encoder.layer.5.intermediate.dense.bias
encoder.layer.5.output.dense.weight
encoder.layer.5.output.dense.bias
encoder.layer.5.output.LayerNorm.weight
encoder.layer.5.output.LayerNorm.bias
encoder.layer.6.attention.self.query.weight
encoder.layer.6.attention.self.query.bias
encoder.layer.6.attention.self.key.weight
encoder.layer.6.attention.self.key.bias
encoder.layer.6.attention.self.value.weight
encoder.layer.6.attention.self.value.bias
encoder.layer.6.attention.output.dense.weight
encoder.layer.6.attention.output.dense.bias
encoder.layer.6.attention.output.LayerNorm.weight
encoder.layer.6.attention.output.LayerNorm.bias
encoder.layer.6.intermediate.dense.weight
encoder.layer.6.intermediate.dense.bias
encoder.layer.6.output.dense.weight
encoder.layer.6.output.dense.bias
encoder.layer.6.output.LayerNorm.weight
encoder.layer.6.output.LayerNorm.bias
encoder.layer.7.attention.self.query.weight
encoder.layer.7.attention.self.query.bias
encoder.layer.7.attention.self.key.weight
encoder.layer.7.attention.self.key.bias
encoder.layer.7.attention.self.value.weight
encoder.layer.7.attention.self.value.bias
encoder.layer.7.attention.output.dense.weight
encoder.layer.7.attention.output.dense.bias
encoder.layer.7.attention.output.LayerNorm.weight
encoder.layer.7.attention.output.LayerNorm.bias
encoder.layer.7.intermediate.dense.weight
encoder.layer.7.intermediate.dense.bias
encoder.layer.7.output.dense.weight
encoder.layer.7.output.dense.bias
encoder.layer.7.output.LayerNorm.weight
encoder.layer.7.output.LayerNorm.bias
encoder.layer.8.attention.self.query.weight
encoder.layer.8.attention.self.query.bias
encoder.layer.8.attention.self.key.weight
encoder.layer.8.attention.self.key.bias
encoder.layer.8.attention.self.value.weight
encoder.layer.8.attention.self.value.bias
encoder.layer.8.attention.output.dense.weight
encoder.layer.8.attention.output.dense.bias
encoder.layer.8.attention.output.LayerNorm.weight
encoder.layer.8.attention.output.LayerNorm.bias
encoder.layer.8.intermediate.dense.weight
encoder.layer.8.intermediate.dense.bias
encoder.layer.8.output.dense.weight
encoder.layer.8.output.dense.bias
encoder.layer.8.output.LayerNorm.weight
encoder.layer.8.output.LayerNorm.bias
encoder.layer.9.attention.self.query.weight
encoder.layer.9.attention.self.query.bias
encoder.layer.9.attention.self.key.weight
encoder.layer.9.attention.self.key.bias
encoder.layer.9.attention.self.value.weight
encoder.layer.9.attention.self.value.bias
encoder.layer.9.attention.output.dense.weight
encoder.layer.9.attention.output.dense.bias
encoder.layer.9.attention.output.LayerNorm.weight
encoder.layer.9.attention.output.LayerNorm.bias
encoder.layer.9.intermediate.dense.weight
encoder.layer.9.intermediate.dense.bias
encoder.layer.9.output.dense.weight
encoder.layer.9.output.dense.bias
encoder.layer.9.output.LayerNorm.weight
encoder.layer.9.output.LayerNorm.bias
encoder.layer.10.attention.self.query.weight
encoder.layer.10.attention.self.query.bias
encoder.layer.10.attention.self.key.weight
encoder.layer.10.attention.self.key.bias
encoder.layer.10.attention.self.value.weight
encoder.layer.10.attention.self.value.bias
encoder.layer.10.attention.output.dense.weight
encoder.layer.10.attention.output.dense.bias
encoder.layer.10.attention.output.LayerNorm.weight
encoder.layer.10.attention.output.LayerNorm.bias
encoder.layer.10.intermediate.dense.weight
encoder.layer.10.intermediate.dense.bias
encoder.layer.10.output.dense.weight
encoder.layer.10.output.dense.bias
encoder.layer.10.output.LayerNorm.weight
encoder.layer.10.output.LayerNorm.bias
encoder.layer.11.attention.self.query.weight
Start running train at 09-03 02:50:35
layers in gcn:  3
{'loss': 0.6851, 'grad_norm': 1.7517434358596802, 'learning_rate': 3.35345405767941e-05, 'epoch': 0.06}
{'loss': 0.6852, 'grad_norm': 0.7877062559127808, 'learning_rate': 6.70690811535882e-05, 'epoch': 0.12}
{'loss': 0.6592, 'grad_norm': 2.9614977836608887, 'learning_rate': 9.986851716581447e-05, 'epoch': 0.18}
{'loss': 0.6509, 'grad_norm': 1.0079551935195923, 'learning_rate': 9.256391526661798e-05, 'epoch': 0.24}
{'loss': 0.6696, 'grad_norm': 1.8755857944488525, 'learning_rate': 8.525931336742148e-05, 'epoch': 0.3}
{'loss': 0.6546, 'grad_norm': 0.5918239951133728, 'learning_rate': 7.795471146822498e-05, 'epoch': 0.36}
{'loss': 0.6548, 'grad_norm': 1.309243083000183, 'learning_rate': 7.065010956902849e-05, 'epoch': 0.42}
{'loss': 0.6439, 'grad_norm': 0.9076733589172363, 'learning_rate': 6.334550766983199e-05, 'epoch': 0.48}
{'loss': 0.6854, 'grad_norm': 2.177408218383789, 'learning_rate': 5.6040905770635496e-05, 'epoch': 0.54}
{'loss': 0.6582, 'grad_norm': 2.5320823192596436, 'learning_rate': 4.873630387143901e-05, 'epoch': 0.6}
{'loss': 0.6313, 'grad_norm': 2.8289148807525635, 'learning_rate': 4.143170197224252e-05, 'epoch': 0.66}
{'loss': 0.6359, 'grad_norm': 2.9264461994171143, 'learning_rate': 3.412710007304602e-05, 'epoch': 0.72}
{'loss': 0.6166, 'grad_norm': 1.5098011493682861, 'learning_rate': 2.6822498173849525e-05, 'epoch': 0.78}
{'loss': 0.6358, 'grad_norm': 2.3674111366271973, 'learning_rate': 1.9517896274653033e-05, 'epoch': 0.84}
{'loss': 0.6715, 'grad_norm': 2.6393826007843018, 'learning_rate': 1.2213294375456538e-05, 'epoch': 0.9}
{'loss': 0.6305, 'grad_norm': 4.895550727844238, 'learning_rate': 4.908692476260044e-06, 'epoch': 0.96}
{'train_runtime': 173.7916, 'train_samples_per_second': 47.965, 'train_steps_per_second': 47.965, 'train_loss': 0.6539330729596217, 'epoch': 1.0}
LM saved to prt_lm/cora/bert-base-uncased-seed0.ckpt
Finished running train at 09-03 02:53:30, running time = 2.91min.
Start running eval_and_save at 09-03 02:53:30
tensor([[-0.7411,  0.7947,  0.7945,  ..., -0.7455,  0.7850,  0.7880],
        [-0.3640,  0.3784,  0.3825,  ..., -0.3833,  0.3780,  0.3546],
        [-0.5456,  0.5741,  0.5842,  ..., -0.5608,  0.5780,  0.5746],
        ...,
        [-0.6219,  0.6435,  0.6449,  ..., -0.6524,  0.6395,  0.6181],
        [-0.5882,  0.6087,  0.6245,  ..., -0.6301,  0.6138,  0.5812],
        [-0.6883,  0.7229,  0.7376,  ..., -0.7215,  0.7253,  0.7043]],
       device='cuda:0')
None

============================= JOB FEEDBACK =============================

Job ID: 2620169
Cluster: hk
User/Group: cc7738/hk-project-test-p0021478
Account: hk-project-pai00001
State: FAILED (exit code 1)
Partition: accelerated
Nodes: 1
Cores per node: 152
Nodelist: hkn0521
CPU Utilized: 00:03:08
CPU Efficiency: 0.55% of 09:30:00 core-walltime
Job Wall-clock time: 00:03:45
Starttime: Mon Sep  2 20:49:47 2024
Endtime: Mon Sep  2 20:53:32 2024
Memory Utilized: 1.63 GB
Memory Efficiency: 3.32% of 48.98 GB
Energy Consumed: 135888 Joule / 37.7466666666667 Watthours
Average node power draw: 603.946666666667 Watt
