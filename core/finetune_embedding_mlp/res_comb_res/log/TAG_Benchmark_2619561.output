Number of available CUDA devices: 1
Number of available CUDA devices: 1
not loaded 0 papers.
not loaded 0 paperid.
create graph: 0.07412958145141602
[2485]
original num of nodes: 2485
num of nodes after lcc: 2485
num of edges after lcc: 10418
num of texts in dataset: 2485
train adj shape: 8336
train: original length 4168
train: downsampled length 4168
valid: original length 781
valid: downsampled length 781
test: original length 260
test: downsampled length 260
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.token_type_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias
encoder.layer.0.attention.self.query.weight
encoder.layer.0.attention.self.query.bias
encoder.layer.0.attention.self.key.weight
encoder.layer.0.attention.self.key.bias
encoder.layer.0.attention.self.value.weight
encoder.layer.0.attention.self.value.bias
encoder.layer.0.attention.output.dense.weight
encoder.layer.0.attention.output.dense.bias
encoder.layer.0.attention.output.LayerNorm.weight
encoder.layer.0.attention.output.LayerNorm.bias
encoder.layer.0.intermediate.dense.weight
encoder.layer.0.intermediate.dense.bias
encoder.layer.0.output.dense.weight
encoder.layer.0.output.dense.bias
encoder.layer.0.output.LayerNorm.weight
encoder.layer.0.output.LayerNorm.bias
encoder.layer.1.attention.self.query.weight
encoder.layer.1.attention.self.query.bias
encoder.layer.1.attention.self.key.weight
encoder.layer.1.attention.self.key.bias
encoder.layer.1.attention.self.value.weight
encoder.layer.1.attention.self.value.bias
encoder.layer.1.attention.output.dense.weight
encoder.layer.1.attention.output.dense.bias
encoder.layer.1.attention.output.LayerNorm.weight
encoder.layer.1.attention.output.LayerNorm.bias
encoder.layer.1.intermediate.dense.weight
encoder.layer.1.intermediate.dense.bias
encoder.layer.1.output.dense.weight
encoder.layer.1.output.dense.bias
encoder.layer.1.output.LayerNorm.weight
encoder.layer.1.output.LayerNorm.bias
encoder.layer.2.attention.self.query.weight
encoder.layer.2.attention.self.query.bias
encoder.layer.2.attention.self.key.weight
encoder.layer.2.attention.self.key.bias
encoder.layer.2.attention.self.value.weight
encoder.layer.2.attention.self.value.bias
encoder.layer.2.attention.output.dense.weight
encoder.layer.2.attention.output.dense.bias
encoder.layer.2.attention.output.LayerNorm.weight
encoder.layer.2.attention.output.LayerNorm.bias
encoder.layer.2.intermediate.dense.weight
encoder.layer.2.intermediate.dense.bias
encoder.layer.2.output.dense.weight
encoder.layer.2.output.dense.bias
encoder.layer.2.output.LayerNorm.weight
encoder.layer.2.output.LayerNorm.bias
encoder.layer.3.attention.self.query.weight
encoder.layer.3.attention.self.query.bias
encoder.layer.3.attention.self.key.weight
encoder.layer.3.attention.self.key.bias
encoder.layer.3.attention.self.value.weight
encoder.layer.3.attention.self.value.bias
encoder.layer.3.attention.output.dense.weight
encoder.layer.3.attention.output.dense.bias
encoder.layer.3.attention.output.LayerNorm.weight
encoder.layer.3.attention.output.LayerNorm.bias
encoder.layer.3.intermediate.dense.weight
encoder.layer.3.intermediate.dense.bias
encoder.layer.3.output.dense.weight
encoder.layer.3.output.dense.bias
encoder.layer.3.output.LayerNorm.weight
encoder.layer.3.output.LayerNorm.bias
encoder.layer.4.attention.self.query.weight
encoder.layer.4.attention.self.query.bias
encoder.layer.4.attention.self.key.weight
encoder.layer.4.attention.self.key.bias
encoder.layer.4.attention.self.value.weight
encoder.layer.4.attention.self.value.bias
encoder.layer.4.attention.output.dense.weight
encoder.layer.4.attention.output.dense.bias
encoder.layer.4.attention.output.LayerNorm.weight
encoder.layer.4.attention.output.LayerNorm.bias
encoder.layer.4.intermediate.dense.weight
encoder.layer.4.intermediate.dense.bias
encoder.layer.4.output.dense.weight
encoder.layer.4.output.dense.bias
encoder.layer.4.output.LayerNorm.weight
encoder.layer.4.output.LayerNorm.bias
encoder.layer.5.attention.self.query.weight
encoder.layer.5.attention.self.query.bias
encoder.layer.5.attention.self.key.weight
encoder.layer.5.attention.self.key.bias
encoder.layer.5.attention.self.value.weight
encoder.layer.5.attention.self.value.bias
encoder.layer.5.attention.output.dense.weight
encoder.layer.5.attention.output.dense.bias
encoder.layer.5.attention.output.LayerNorm.weight
encoder.layer.5.attention.output.LayerNorm.bias
encoder.layer.5.intermediate.dense.weight
encoder.layer.5.intermediate.dense.bias
encoder.layer.5.output.dense.weight
encoder.layer.5.output.dense.bias
encoder.layer.5.output.LayerNorm.weight
encoder.layer.5.output.LayerNorm.bias
encoder.layer.6.attention.self.query.weight
encoder.layer.6.attention.self.query.bias
encoder.layer.6.attention.self.key.weight
encoder.layer.6.attention.self.key.bias
encoder.layer.6.attention.self.value.weight
encoder.layer.6.attention.self.value.bias
encoder.layer.6.attention.output.dense.weight
encoder.layer.6.attention.output.dense.bias
encoder.layer.6.attention.output.LayerNorm.weight
encoder.layer.6.attention.output.LayerNorm.bias
encoder.layer.6.intermediate.dense.weight
encoder.layer.6.intermediate.dense.bias
encoder.layer.6.output.dense.weight
encoder.layer.6.output.dense.bias
encoder.layer.6.output.LayerNorm.weight
encoder.layer.6.output.LayerNorm.bias
encoder.layer.7.attention.self.query.weight
encoder.layer.7.attention.self.query.bias
encoder.layer.7.attention.self.key.weight
encoder.layer.7.attention.self.key.bias
encoder.layer.7.attention.self.value.weight
encoder.layer.7.attention.self.value.bias
encoder.layer.7.attention.output.dense.weight
encoder.layer.7.attention.output.dense.bias
encoder.layer.7.attention.output.LayerNorm.weight
encoder.layer.7.attention.output.LayerNorm.bias
encoder.layer.7.intermediate.dense.weight
encoder.layer.7.intermediate.dense.bias
encoder.layer.7.output.dense.weight
encoder.layer.7.output.dense.bias
encoder.layer.7.output.LayerNorm.weight
encoder.layer.7.output.LayerNorm.bias
encoder.layer.8.attention.self.query.weight
encoder.layer.8.attention.self.query.bias
encoder.layer.8.attention.self.key.weight
encoder.layer.8.attention.self.key.bias
encoder.layer.8.attention.self.value.weight
encoder.layer.8.attention.self.value.bias
encoder.layer.8.attention.output.dense.weight
encoder.layer.8.attention.output.dense.bias
encoder.layer.8.attention.output.LayerNorm.weight
encoder.layer.8.attention.output.LayerNorm.bias
encoder.layer.8.intermediate.dense.weight
encoder.layer.8.intermediate.dense.bias
encoder.layer.8.output.dense.weight
encoder.layer.8.output.dense.bias
encoder.layer.8.output.LayerNorm.weight
encoder.layer.8.output.LayerNorm.bias
encoder.layer.9.attention.self.query.weight
encoder.layer.9.attention.self.query.bias
encoder.layer.9.attention.self.key.weight
encoder.layer.9.attention.self.key.bias
encoder.layer.9.attention.self.value.weight
encoder.layer.9.attention.self.value.bias
encoder.layer.9.attention.output.dense.weight
encoder.layer.9.attention.output.dense.bias
encoder.layer.9.attention.output.LayerNorm.weight
encoder.layer.9.attention.output.LayerNorm.bias
encoder.layer.9.intermediate.dense.weight
encoder.layer.9.intermediate.dense.bias
encoder.layer.9.output.dense.weight
encoder.layer.9.output.dense.bias
encoder.layer.9.output.LayerNorm.weight
encoder.layer.9.output.LayerNorm.bias
encoder.layer.10.attention.self.query.weight
encoder.layer.10.attention.self.query.bias
encoder.layer.10.attention.self.key.weight
encoder.layer.10.attention.self.key.bias
encoder.layer.10.attention.self.value.weight
encoder.layer.10.attention.self.value.bias
encoder.layer.10.attention.output.dense.weight
encoder.layer.10.attention.output.dense.bias
encoder.layer.10.attention.output.LayerNorm.weight
encoder.layer.10.attention.output.LayerNorm.bias
encoder.layer.10.intermediate.dense.weight
encoder.layer.10.intermediate.dense.bias
encoder.layer.10.output.dense.weight
encoder.layer.10.output.dense.bias
encoder.layer.10.output.LayerNorm.weight
encoder.layer.10.output.LayerNorm.bias
encoder.layer.11.attention.self.query.weight
Start running train at 09-02 20:33:52
[2024-09-02 14:33:53,152] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)

============================= JOB FEEDBACK =============================

Job ID: 2619561
Cluster: hk
User/Group: cc7738/hk-project-test-p0021478
Account: hk-project-pai00001
State: FAILED (exit code 1)
Partition: accelerated
Nodes: 1
Cores per node: 152
Nodelist: hkn0734
CPU Utilized: 00:00:20
CPU Efficiency: 0.26% of 02:09:12 core-walltime
Job Wall-clock time: 00:00:51
Starttime: Mon Sep  2 14:33:05 2024
Endtime: Mon Sep  2 14:33:56 2024
Memory Utilized: 1.09 GB
Memory Efficiency: 2.23% of 48.98 GB
Energy Consumed: 17199 Joule / 4.7775 Watthours
Average node power draw: 337.235294117647 Watt
