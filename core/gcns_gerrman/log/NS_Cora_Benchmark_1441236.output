Namespace(cfg_file='core/yamls/pubmed/gcns/ns_gnn_models.yaml', sweep_file='core/yamls/cora/gcns/gae_sp1.yaml', data='pubmed', bs=32768, device='cuda:0', epochs=200, model='GAT', score='mlp_score', wandb=None, repeat=3, mark_done=False, opts=[])
DDEVICE:  cuda:0 cuda:0
cuda
Number of available CUDA devices: 4
Number of available CUDA devices: 4
dropout: 0
emb: False
heads: 1
hidden_channels: 256
in_channels: None
negative_slope: 0.2
num_layers: 3
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
layers in gat:  3
Epoch: 1, Loss: 0.6854504942893982
Epoch: 2, Loss: 0.6040799021720886
Epoch: 3, Loss: 0.5180945992469788
Epoch: 4, Loss: 0.4934847056865692
Epoch: 5, Loss: 0.45137131214141846
Epoch: 6, Loss: 0.41650086641311646
Epoch: 7, Loss: 0.4063141942024231
Epoch: 8, Loss: 0.4352256953716278
Epoch: 9, Loss: 0.4055039584636688
Epoch: 10, Loss: 0.3952670693397522
Epoch: 11, Loss: 0.3534349203109741
Epoch: 12, Loss: 0.37248459458351135
Epoch: 13, Loss: 0.3164447546005249
Epoch: 14, Loss: 0.3031226098537445
Epoch: 15, Loss: 0.32008183002471924
Epoch: 16, Loss: 0.28082484006881714
Epoch: 17, Loss: 0.3112347424030304
Epoch: 18, Loss: 0.2982620596885681
Epoch: 19, Loss: 0.32253241539001465
Epoch: 20, Loss: 0.28212395310401917
Epoch: 21, Loss: 0.2602269947528839
Epoch: 22, Loss: 0.3173392713069916
Epoch: 23, Loss: 0.25094330310821533
Epoch: 24, Loss: 0.24325351417064667
Epoch: 25, Loss: 0.2420816421508789
Epoch: 26, Loss: 0.23955486714839935
Epoch: 27, Loss: 0.23917916417121887
Epoch: 28, Loss: 0.25055789947509766
Epoch: 29, Loss: 0.2198362648487091
Epoch: 30, Loss: 0.23124589025974274
Epoch: 31, Loss: 0.24272507429122925
Epoch: 32, Loss: 0.22889772057533264
Epoch: 33, Loss: 0.2718661427497864
Epoch: 34, Loss: 0.23343592882156372
Epoch: 35, Loss: 0.22233977913856506
Epoch: 36, Loss: 0.22576060891151428
Epoch: 37, Loss: 0.18809840083122253
Epoch: 38, Loss: 0.19727560877799988
Epoch: 39, Loss: 0.21811090409755707
Epoch: 40, Loss: 0.2312166541814804
Epoch: 41, Loss: 0.18739813566207886
Epoch: 42, Loss: 0.20751386880874634
Epoch: 43, Loss: 0.21089404821395874
Epoch: 44, Loss: 0.2037208527326584
Epoch: 45, Loss: 0.26402634382247925
Epoch: 46, Loss: 0.2116965502500534
Epoch: 47, Loss: 0.1749066263437271
Epoch: 48, Loss: 0.20217019319534302
Epoch: 49, Loss: 0.22414380311965942
Epoch: 50, Loss: 0.20723462104797363
Epoch: 51, Loss: 0.17566195130348206
Epoch: 52, Loss: 0.19930879771709442
Epoch: 53, Loss: 0.21484717726707458
Epoch: 54, Loss: 0.2096991240978241
Epoch: 55, Loss: 0.1875573843717575
Epoch: 56, Loss: 0.2759947180747986
Epoch: 57, Loss: 0.215852290391922
Epoch: 58, Loss: 0.19451096653938293
Epoch: 59, Loss: 0.19086317718029022
Epoch: 60, Loss: 0.19679231941699982
Epoch: 61, Loss: 0.16577255725860596
Epoch: 62, Loss: 0.21897895634174347
Epoch: 63, Loss: 0.20761097967624664
Epoch: 64, Loss: 0.1955869346857071
Epoch: 65, Loss: 0.2019023597240448
Epoch: 66, Loss: 0.23769250512123108
Epoch: 67, Loss: 0.18122214078903198
Epoch: 68, Loss: 0.22644922137260437
Epoch: 69, Loss: 0.23072318732738495
Epoch: 70, Loss: 0.21556183695793152
Epoch: 71, Loss: 0.32672053575515747
Epoch: 72, Loss: 0.22134581208229065
Epoch: 73, Loss: 0.41157591342926025
Epoch: 74, Loss: 0.13876491785049438
Epoch: 75, Loss: 0.23131127655506134
Epoch: 76, Loss: 0.22580662369728088
Epoch: 77, Loss: 0.2641294300556183
Epoch: 78, Loss: 0.26729440689086914
Epoch: 79, Loss: 0.2666570544242859
Epoch: 80, Loss: 0.1764378398656845
Epoch: 81, Loss: 0.21333175897598267
Epoch: 82, Loss: 0.23493286967277527
Epoch: 83, Loss: 0.21735869348049164
Epoch: 84, Loss: 0.2673678398132324
Epoch: 85, Loss: 0.2992667257785797
Epoch: 86, Loss: 0.21148084104061127
Epoch: 87, Loss: 0.23067089915275574
Epoch: 88, Loss: 0.3461638391017914
Epoch: 89, Loss: 0.22750616073608398
Epoch: 90, Loss: 0.27535122632980347
Epoch: 91, Loss: 0.22798649966716766
Epoch: 92, Loss: 0.2396111935377121
Epoch: 93, Loss: 0.3880855441093445
Epoch: 94, Loss: 0.2337583303451538
Epoch: 95, Loss: 0.23660781979560852
Epoch: 96, Loss: 0.20918184518814087
Epoch: 97, Loss: 0.18978437781333923
Epoch: 98, Loss: 0.2872680127620697
Epoch: 99, Loss: 0.26010823249816895
Epoch: 100, Loss: 0.24462832510471344
Epoch: 101, Loss: 0.2731848955154419
Epoch: 102, Loss: 0.21366089582443237
Epoch: 103, Loss: 0.2653564214706421
Epoch: 104, Loss: 0.3683892488479614
Epoch: 105, Loss: 0.21958070993423462
Epoch: 106, Loss: 0.2505139112472534
Epoch: 107, Loss: 0.2297746241092682
Epoch: 108, Loss: 0.2674546241760254
Epoch: 109, Loss: 0.26812744140625
Epoch: 110, Loss: 0.23314788937568665
Epoch: 111, Loss: 0.25059595704078674
Epoch: 112, Loss: 0.29400116205215454
Epoch: 113, Loss: 0.2762836217880249
Epoch: 114, Loss: 0.30471521615982056
Epoch: 115, Loss: 0.24521788954734802
Epoch: 116, Loss: 0.23608613014221191
Epoch: 117, Loss: 0.20954610407352448
Epoch: 118, Loss: 0.18601268529891968
Epoch: 119, Loss: 0.2112402319908142
Epoch: 120, Loss: 0.2027161717414856
Epoch: 121, Loss: 0.2201422154903412
Epoch: 122, Loss: 0.268117755651474
Epoch: 123, Loss: 0.19280630350112915
Epoch: 124, Loss: 0.19608616828918457
Epoch: 125, Loss: 0.15300074219703674
Epoch: 126, Loss: 0.1919425129890442
Epoch: 127, Loss: 0.1741175651550293
Epoch: 128, Loss: 0.24913981556892395
Epoch: 129, Loss: 0.20560546219348907
Epoch: 130, Loss: 0.3092515766620636
Epoch: 131, Loss: 0.20300531387329102
Epoch: 132, Loss: 0.20238173007965088
Epoch: 133, Loss: 0.23599641025066376
Epoch: 134, Loss: 0.2093283236026764
Epoch: 135, Loss: 0.18035632371902466
Epoch: 136, Loss: 0.16636627912521362
Epoch: 137, Loss: 0.22337591648101807
Epoch: 138, Loss: 0.1882161945104599
Epoch: 139, Loss: 0.15297888219356537
Epoch: 140, Loss: 0.1775761842727661
Epoch: 141, Loss: 0.18963466584682465
Epoch: 142, Loss: 0.20164570212364197
Epoch: 143, Loss: 0.16246062517166138
Epoch: 144, Loss: 0.19920751452445984
Epoch: 145, Loss: 0.25748616456985474
Epoch: 146, Loss: 0.1859835535287857
Epoch: 147, Loss: 0.24391332268714905
Epoch: 148, Loss: 0.15879079699516296
Epoch: 149, Loss: 0.2281230390071869
Epoch: 150, Loss: 0.20701131224632263
Epoch: 151, Loss: 0.1562282145023346
Epoch: 152, Loss: 0.19161060452461243
Epoch: 153, Loss: 0.1718726009130478
Epoch: 154, Loss: 0.14954736828804016
Epoch: 155, Loss: 0.15460911393165588
Epoch: 156, Loss: 0.23347820341587067
Epoch: 157, Loss: 0.22056610882282257
Epoch: 158, Loss: 0.21456892788410187
Epoch: 159, Loss: 0.11351987719535828
Epoch: 160, Loss: 0.17464187741279602
Epoch: 161, Loss: 0.1999315619468689
Epoch: 162, Loss: 0.15658307075500488
Epoch: 163, Loss: 0.18190625309944153
Epoch: 164, Loss: 0.1567622572183609
Epoch: 165, Loss: 0.13728982210159302
Epoch: 166, Loss: 0.1446870118379593
Epoch: 167, Loss: 0.15682294964790344
Epoch: 168, Loss: 0.20135021209716797
Epoch: 169, Loss: 0.15262781083583832
Epoch: 170, Loss: 0.15954585373401642
Epoch: 171, Loss: 0.1649617850780487
Epoch: 172, Loss: 0.12594003975391388
Epoch: 173, Loss: 0.19780278205871582
Epoch: 174, Loss: 0.08887419104576111
Epoch: 175, Loss: 0.1271522492170334
Epoch: 176, Loss: 0.1396777480840683
Epoch: 177, Loss: 0.16238410770893097
Epoch: 178, Loss: 0.10730069130659103
Epoch: 179, Loss: 0.12981900572776794
Epoch: 180, Loss: 0.10019564628601074
Epoch: 181, Loss: 0.13086755573749542
Epoch: 182, Loss: 0.14663656055927277
Epoch: 183, Loss: 0.1417332887649536
Epoch: 184, Loss: 0.14258088171482086
Epoch: 185, Loss: 0.15670159459114075
Epoch: 186, Loss: 0.11926159262657166
Epoch: 187, Loss: 0.10393553972244263
Epoch: 188, Loss: 0.14395380020141602
Epoch: 189, Loss: 0.08843319863080978
Epoch: 190, Loss: 0.08586752414703369
Epoch: 191, Loss: 0.14982068538665771
Epoch: 192, Loss: 0.0860634297132492
Epoch: 193, Loss: 0.1804751455783844
Epoch: 194, Loss: 0.10530409216880798
Epoch: 195, Loss: 0.0774250328540802
Epoch: 196, Loss: 0.13573050498962402
Epoch: 197, Loss: 0.10795997828245163
Epoch: 198, Loss: 0.0583307221531868
Epoch: 199, Loss: 0.1155150756239891
Epoch: 200, Loss: 0.14173060655593872
Number of available CUDA devices: 4
Number of available CUDA devices: 4
batch_size_sampler: 128
dropout: 0
emb: False
heads: 4
hidden_channels: 128
in_channels: 500
negative_slope: 0.1
num_hops: 5
num_layers: 3
num_neighbors: 10
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml

============================= JOB FEEDBACK =============================

Job ID: 1441236
Cluster: haic
User/Group: cc7738/aifb
Account: aifb
State: TIMEOUT (exit code 0)
Partition: normal
Nodes: 1
Cores per node: 2
Nodelist: haicn1708
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 16:00:10 core-walltime
Job Wall-clock time: 08:00:05
Starttime: Sat Jun 15 07:50:35 2024
Endtime: Sat Jun 15 15:50:40 2024
Memory Utilized: 20.07 GB
Memory Efficiency: 4.10% of 489.84 GB
Energy Consumed: 16454585 Joule / 4570.71805555556 Watthours
Average node power draw: 571.240583232078 Watt
