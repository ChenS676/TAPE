Namespace(cfg_file='core/yamls/pubmed/gcns/ns_gnn_models.yaml', sweep_file='core/yamls/cora/gcns/gae_sp1.yaml', data='pubmed', bs=32768, device='cuda:0', epochs=100, model='GAE', score='mlp_score', wandb=None, repeat=3, mark_done=False, opts=[])
DDEVICE:  cuda:0 cuda:0
cuda
Number of available CUDA devices: 4
Number of available CUDA devices: 4
dropout: 0
hidden_channels: 256
in_channels: None
negative_slope: 0.2
num_layers: 3
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
Epoch: 1, Loss: 1.0070531368255615
Epoch: 2, Loss: 0.9602842330932617
Epoch: 3, Loss: 0.8956239819526672
Epoch: 4, Loss: 0.8448847532272339
Epoch: 5, Loss: 0.877716600894928
Epoch: 6, Loss: 0.8390288352966309
Epoch: 7, Loss: 0.8238908648490906
Epoch: 8, Loss: 0.8309621810913086
Epoch: 9, Loss: 0.8162606358528137
Epoch: 10, Loss: 0.7690296173095703
Epoch: 11, Loss: 0.7996644377708435
Epoch: 12, Loss: 0.8296118974685669
Epoch: 13, Loss: 0.7808544039726257
Epoch: 14, Loss: 0.7876246571540833
Epoch: 15, Loss: 0.8227481245994568
Epoch: 16, Loss: 0.7853154540061951
Epoch: 17, Loss: 0.7804360389709473
Epoch: 18, Loss: 0.8095145225524902
Epoch: 19, Loss: 0.8045900464057922
Epoch: 20, Loss: 0.7816430330276489
Epoch: 21, Loss: 0.8055466413497925
Epoch: 22, Loss: 0.7749691009521484
Epoch: 23, Loss: 0.7395714521408081
Epoch: 24, Loss: 0.8011954426765442
Epoch: 25, Loss: 0.7987083196640015
Epoch: 26, Loss: 0.7804927825927734
Epoch: 27, Loss: 0.7999638915061951
Epoch: 28, Loss: 0.726328432559967
Epoch: 29, Loss: 0.8375823497772217
Epoch: 30, Loss: 0.7428829073905945
Epoch: 31, Loss: 0.7479162216186523
Epoch: 32, Loss: 0.7112868428230286
Epoch: 33, Loss: 0.7380638718605042
Epoch: 34, Loss: 0.7424950003623962
Epoch: 35, Loss: 0.7584339380264282
Epoch: 36, Loss: 0.7337347865104675
Epoch: 37, Loss: 0.6887187957763672
Epoch: 38, Loss: 0.7932493686676025
Epoch: 39, Loss: 0.6933778524398804
Epoch: 40, Loss: 0.8368723392486572
Epoch: 41, Loss: 0.7160195112228394
Epoch: 42, Loss: 0.7300440669059753
Epoch: 43, Loss: 0.7954838871955872
Epoch: 44, Loss: 0.7004948258399963
Epoch: 45, Loss: 0.7248948812484741
Epoch: 46, Loss: 0.7218891978263855
Epoch: 47, Loss: 0.7139759659767151
Epoch: 48, Loss: 0.8069944977760315
Epoch: 49, Loss: 0.7362873554229736
Epoch: 50, Loss: 0.7223950028419495
Epoch: 51, Loss: 0.7216037511825562
Epoch: 52, Loss: 0.7563056945800781
Epoch: 53, Loss: 0.7025752663612366
Epoch: 54, Loss: 0.6601753830909729
Epoch: 55, Loss: 0.7585370540618896
Epoch: 56, Loss: 0.6660743951797485
Epoch: 57, Loss: 0.7952629923820496
Epoch: 58, Loss: 0.845507800579071
Epoch: 59, Loss: 0.7087643146514893
Epoch: 60, Loss: 0.9190200567245483
Epoch: 61, Loss: 0.646412193775177
Epoch: 62, Loss: 0.78135085105896
Epoch: 63, Loss: 0.7104875445365906
Epoch: 64, Loss: 0.759652853012085
Epoch: 65, Loss: 0.7673789858818054
Epoch: 66, Loss: 0.6954841613769531
Epoch: 67, Loss: 0.7073392271995544
Epoch: 68, Loss: 0.6707659363746643
Epoch: 69, Loss: 0.7172083854675293
Epoch: 70, Loss: 0.7247345447540283
Epoch: 71, Loss: 0.721962034702301
Epoch: 72, Loss: 0.8446664214134216
Epoch: 73, Loss: 0.8196514248847961
Epoch: 74, Loss: 0.7777261734008789
Epoch: 75, Loss: 0.7903974056243896
Epoch: 76, Loss: 0.708333432674408
Epoch: 77, Loss: 0.7218242287635803
Epoch: 78, Loss: 0.8211418986320496
Epoch: 79, Loss: 0.7265169620513916
Epoch: 80, Loss: 0.740158200263977
Epoch: 81, Loss: 0.8985317349433899
Epoch: 82, Loss: 0.7129878997802734
Epoch: 83, Loss: 0.7664753198623657
Epoch: 84, Loss: 0.8437028527259827
Epoch: 85, Loss: 0.7875684499740601
Epoch: 86, Loss: 0.8870776891708374
Epoch: 87, Loss: 0.6974198818206787
Epoch: 88, Loss: 0.6931530237197876
Epoch: 89, Loss: 0.6506716012954712
Epoch: 90, Loss: 0.659758448600769
Epoch: 91, Loss: 0.8394373059272766
Epoch: 92, Loss: 0.6487923264503479
Epoch: 93, Loss: 0.9589878916740417
Epoch: 94, Loss: 0.849917471408844
Epoch: 95, Loss: 0.742557942867279
Epoch: 96, Loss: 0.76829594373703
Epoch: 97, Loss: 0.7150281667709351
Epoch: 98, Loss: 0.7308476567268372
Epoch: 99, Loss: 0.7701429724693298
Epoch: 100, Loss: 0.8275910019874573
Number of available CUDA devices: 4
Number of available CUDA devices: 4
batch_size_sampler: 128
dropout: 0
hidden_channels: 128
in_channels: 500
negative_slope: 0.2
num_hops: 5
num_layers: 3
num_neighbors: 10
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
Epoch: 1, Loss: 1.0191681385040283
Epoch: 2, Loss: 0.9710179567337036
Epoch: 3, Loss: 0.9478443264961243
Epoch: 4, Loss: 0.9118884205818176
Epoch: 5, Loss: 0.883578896522522
Epoch: 6, Loss: 0.8560707569122314
Epoch: 7, Loss: 0.8467955589294434
Epoch: 8, Loss: 0.8405373692512512
Epoch: 9, Loss: 0.7977411150932312
Epoch: 10, Loss: 0.8343307971954346
Epoch: 11, Loss: 0.8249101638793945
Epoch: 12, Loss: 0.8317862153053284
Epoch: 13, Loss: 0.8179644346237183
Epoch: 14, Loss: 0.8172615170478821
Epoch: 15, Loss: 0.8688685894012451
Epoch: 16, Loss: 0.8091266751289368
Epoch: 17, Loss: 0.7878208160400391
Epoch: 18, Loss: 0.8284298777580261
Epoch: 19, Loss: 0.7947240471839905
Epoch: 20, Loss: 0.7982752919197083
Epoch: 21, Loss: 0.795179009437561
Epoch: 22, Loss: 0.7879090309143066
Epoch: 23, Loss: 0.8140854239463806
Epoch: 24, Loss: 0.7674216628074646
Epoch: 25, Loss: 0.7147307991981506
Epoch: 26, Loss: 0.7458106875419617
Epoch: 27, Loss: 0.7453885674476624
Epoch: 28, Loss: 0.7621297836303711
Epoch: 29, Loss: 0.8066495060920715
Epoch: 30, Loss: 0.7135204076766968
Epoch: 31, Loss: 0.7411569952964783
Epoch: 32, Loss: 0.7585709691047668
Epoch: 33, Loss: 0.8060387969017029
Epoch: 34, Loss: 0.772864818572998
Epoch: 35, Loss: 0.7914045453071594
Epoch: 36, Loss: 0.7758055329322815
Epoch: 37, Loss: 0.7424768209457397
Epoch: 38, Loss: 0.7334216833114624
Epoch: 39, Loss: 0.7497550845146179
Epoch: 40, Loss: 0.7030905485153198
Epoch: 41, Loss: 0.7340908646583557
Epoch: 42, Loss: 0.7243271470069885
Epoch: 43, Loss: 0.7104296684265137
Epoch: 44, Loss: 0.7781039476394653
Epoch: 45, Loss: 0.786676287651062
Epoch: 46, Loss: 0.7116961479187012
Epoch: 47, Loss: 0.729830265045166
Epoch: 48, Loss: 0.7824312448501587
Epoch: 49, Loss: 0.7056648135185242
Epoch: 50, Loss: 0.7782115340232849
Epoch: 51, Loss: 0.7332914471626282
Epoch: 52, Loss: 0.7705934047698975
Epoch: 53, Loss: 0.6901226043701172
Epoch: 54, Loss: 0.6793281435966492
Epoch: 55, Loss: 0.76521235704422
Epoch: 56, Loss: 0.7547524571418762
Epoch: 57, Loss: 0.8804190158843994
Epoch: 58, Loss: 0.7127880454063416
Epoch: 59, Loss: 0.7860159873962402
Epoch: 60, Loss: 0.6553133130073547
Epoch: 61, Loss: 0.7348668575286865
Epoch: 62, Loss: 0.7196712493896484
Epoch: 63, Loss: 0.7061735987663269
Epoch: 64, Loss: 0.7171463370323181
Epoch: 65, Loss: 0.7228069305419922
Epoch: 66, Loss: 0.7545310258865356
Epoch: 67, Loss: 0.8677718043327332
Epoch: 68, Loss: 0.7121103405952454
Epoch: 69, Loss: 0.8544655442237854
Epoch: 70, Loss: 0.8033241629600525
Epoch: 71, Loss: 0.8077844381332397
Epoch: 72, Loss: 0.6558253169059753
Epoch: 73, Loss: 0.8122542500495911
Epoch: 74, Loss: 0.974260687828064
Epoch: 75, Loss: 0.8359288573265076
Epoch: 76, Loss: 0.7232658267021179
Epoch: 77, Loss: 0.7108851075172424
Epoch: 78, Loss: 0.6476843357086182
Epoch: 79, Loss: 0.7845450043678284
Epoch: 80, Loss: 0.7961078882217407
Epoch: 81, Loss: 0.6973020434379578
Epoch: 82, Loss: 0.6547256708145142
Epoch: 83, Loss: 0.7814099788665771
Epoch: 84, Loss: 0.7754213809967041
Epoch: 85, Loss: 0.731071412563324
Epoch: 86, Loss: 0.7476857900619507
Epoch: 87, Loss: 0.7741946578025818
Epoch: 88, Loss: 0.760361909866333
Epoch: 89, Loss: 0.7015456557273865
Epoch: 90, Loss: 0.7406542301177979
Epoch: 91, Loss: 0.6646673679351807
Epoch: 92, Loss: 0.8988069295883179
Epoch: 93, Loss: 0.8903964757919312
Epoch: 94, Loss: 0.8552192449569702
Epoch: 95, Loss: 0.8897833824157715
Epoch: 96, Loss: 0.7268590927124023
Epoch: 97, Loss: 0.7312632203102112
Epoch: 98, Loss: 0.8070258498191833
Epoch: 99, Loss: 0.9050835967063904
Epoch: 100, Loss: 0.7049375772476196
Number of available CUDA devices: 4
Number of available CUDA devices: 4
batch_size_sampler: 128
dropout: 0
hidden_channels: 128
in_channels: 500
negative_slope: 0.2
num_hops: 5
num_layers: 3
num_neighbors: 10
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
Epoch: 1, Loss: 0.9834051132202148
Epoch: 2, Loss: 0.9773386716842651
Epoch: 3, Loss: 0.8787150382995605
Epoch: 4, Loss: 0.8697478771209717
Epoch: 5, Loss: 0.8808164596557617
Epoch: 6, Loss: 0.8281919360160828
Epoch: 7, Loss: 0.8013294339179993
Epoch: 8, Loss: 0.8051289319992065
Epoch: 9, Loss: 0.8080770373344421
Epoch: 10, Loss: 0.8208209276199341
Epoch: 11, Loss: 0.7746974229812622
Epoch: 12, Loss: 0.7907755374908447
Epoch: 13, Loss: 0.7963995933532715
Epoch: 14, Loss: 0.7362159490585327
Epoch: 15, Loss: 0.7558982968330383
Epoch: 16, Loss: 0.772454023361206
Epoch: 17, Loss: 0.8102417588233948
Epoch: 18, Loss: 0.7410835027694702
Epoch: 19, Loss: 0.7841381430625916
Epoch: 20, Loss: 0.7693740129470825
Epoch: 21, Loss: 0.7301756143569946
Epoch: 22, Loss: 0.7499353289604187
Epoch: 23, Loss: 0.7350481748580933
Epoch: 24, Loss: 0.7172516584396362
Epoch: 25, Loss: 0.7275004982948303
Epoch: 26, Loss: 0.720674991607666
Epoch: 27, Loss: 0.7362394332885742
Epoch: 28, Loss: 0.7315502166748047
Epoch: 29, Loss: 0.6903291344642639
Epoch: 30, Loss: 0.7335767149925232
Epoch: 31, Loss: 0.7504881024360657
Epoch: 32, Loss: 0.7415697574615479
Epoch: 33, Loss: 0.7256889343261719
Epoch: 34, Loss: 0.7143401503562927
Epoch: 35, Loss: 0.6909454464912415
Epoch: 36, Loss: 0.7536038756370544
Epoch: 37, Loss: 0.690437376499176
Epoch: 38, Loss: 0.7249521613121033
Epoch: 39, Loss: 0.7098708748817444
Epoch: 40, Loss: 0.7212737798690796
Epoch: 41, Loss: 0.727918803691864
Epoch: 42, Loss: 0.6948255896568298
Epoch: 43, Loss: 0.7070752382278442
Epoch: 44, Loss: 0.7313451170921326
Epoch: 45, Loss: 0.7423021793365479
Epoch: 46, Loss: 0.6917446851730347
Epoch: 47, Loss: 0.674646258354187
Epoch: 48, Loss: 0.6677209734916687
Epoch: 49, Loss: 0.7387792468070984
Epoch: 50, Loss: 0.7571130990982056
Epoch: 51, Loss: 0.6877953410148621
Epoch: 52, Loss: 0.6881471276283264
Epoch: 53, Loss: 0.6960579752922058
Epoch: 54, Loss: 0.7280807495117188
Epoch: 55, Loss: 0.654382050037384
Epoch: 56, Loss: 0.7308263778686523
Epoch: 57, Loss: 0.7240718007087708
Epoch: 58, Loss: 0.6916399002075195
Epoch: 59, Loss: 0.6674489378929138
Epoch: 60, Loss: 0.7037633657455444
Epoch: 61, Loss: 0.6938736438751221
Epoch: 62, Loss: 0.6558268666267395
Epoch: 63, Loss: 0.7006917595863342
Epoch: 64, Loss: 0.7485689520835876
Epoch: 65, Loss: 0.7392090559005737
Epoch: 66, Loss: 0.6362216472625732
Epoch: 67, Loss: 0.6879947781562805
Epoch: 68, Loss: 0.6767386198043823
Epoch: 69, Loss: 0.6356483697891235
Epoch: 70, Loss: 0.6696513295173645
Epoch: 71, Loss: 0.6386545896530151
Epoch: 72, Loss: 0.688210666179657
Epoch: 73, Loss: 0.6987012028694153
Epoch: 74, Loss: 0.7434967756271362
Epoch: 75, Loss: 0.6641532182693481
Epoch: 76, Loss: 0.6958656311035156
Epoch: 77, Loss: 0.7778750061988831
Epoch: 78, Loss: 0.650692343711853
Epoch: 79, Loss: 0.6768710613250732
Epoch: 80, Loss: 0.6675356030464172
Epoch: 81, Loss: 0.693081259727478
Epoch: 82, Loss: 0.728920042514801
Epoch: 83, Loss: 0.6899200677871704
Epoch: 84, Loss: 0.6328659057617188
Epoch: 85, Loss: 0.6993298530578613
Epoch: 86, Loss: 0.658557653427124
Epoch: 87, Loss: 0.7107028961181641
Epoch: 88, Loss: 0.6996294260025024
Epoch: 89, Loss: 0.8753275275230408
Epoch: 90, Loss: 0.6885083317756653
Epoch: 91, Loss: 0.6826746463775635
Epoch: 92, Loss: 0.7103151679039001
Epoch: 93, Loss: 0.7989400625228882
Epoch: 94, Loss: 0.7177768349647522
Epoch: 95, Loss: 0.6954881548881531
Epoch: 96, Loss: 0.7078139781951904
Epoch: 97, Loss: 0.7940741181373596
Epoch: 98, Loss: 0.774196445941925
Epoch: 99, Loss: 0.6773412823677063
Epoch: 100, Loss: 0.749108612537384
Hits@1
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 13.39 ± 4.34
Highest Valid: 2.83 ± 0.33
Train with the best valid: 4.61 ± 1.03
Test with the best valid epoch: 4.61 ± 1.03
Hits@3
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 29.54 ± 9.86
Highest Valid: 8.47 ± 3.52
Train with the best valid: 7.13 ± 4.26
Test with the best valid epoch: 7.13 ± 4.26
Hits@10
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 49.09 ± 1.48
Highest Valid: 16.28 ± 1.14
Train with the best valid: 11.59 ± 3.12
Test with the best valid epoch: 11.59 ± 3.12
Hits@20
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 61.51 ± 3.78
Highest Valid: 26.84 ± 3.55
Train with the best valid: 17.13 ± 2.71
Test with the best valid epoch: 17.13 ± 2.71
Hits@50
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 77.66 ± 2.58
Highest Valid: 40.82 ± 4.23
Train with the best valid: 27.64 ± 1.28
Test with the best valid epoch: 27.64 ± 1.28
Hits@100
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 87.03 ± 2.77
Highest Valid: 56.78 ± 3.58
Train with the best valid: 40.16 ± 2.38
Test with the best valid epoch: 40.16 ± 2.38
MRR
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 20.89 ± 5.89
Highest Valid: 7.89 ± 1.69
Train with the best valid: 4.99 ± 1.42
Test with the best valid epoch: 4.99 ± 1.42
mrr_hit1
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 13.39 ± 4.34
Highest Valid: 2.83 ± 0.33
Train with the best valid: 4.61 ± 1.03
Test with the best valid epoch: 4.61 ± 1.03
mrr_hit3
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 30.22 ± 8.87
Highest Valid: 8.47 ± 3.52
Train with the best valid: 7.13 ± 4.26
Test with the best valid epoch: 7.13 ± 4.26
mrr_hit10
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 49.09 ± 1.48
Highest Valid: 16.28 ± 1.14
Train with the best valid: 11.59 ± 3.12
Test with the best valid epoch: 11.59 ± 3.12
mrr_hit20
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 61.51 ± 3.78
Highest Valid: 26.84 ± 3.55
Train with the best valid: 17.13 ± 2.71
Test with the best valid epoch: 17.13 ± 2.71
mrr_hit50
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 77.66 ± 2.58
Highest Valid: 40.82 ± 4.23
Train with the best valid: 27.64 ± 1.28
Test with the best valid epoch: 27.64 ± 1.28
mrr_hit100
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 87.03 ± 2.77
Highest Valid: 56.78 ± 3.58
Train with the best valid: 40.16 ± 2.38
Test with the best valid epoch: 40.16 ± 2.38
AUC
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 99.57 ± 0.06
Highest Valid: 95.62 ± 0.46
Train with the best valid: 98.03 ± 0.11
Test with the best valid epoch: 98.03 ± 0.11
AP
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 99.79 ± 0.03
Highest Valid: 97.11 ± 0.34
Train with the best valid: 98.96 ± 0.05
Test with the best valid epoch: 98.96 ± 0.05
ACC
torch.Size([20, 3])
torch.Size([20, 3])
torch.Size([20, 3])
Highest Train: 95.14 ± 0.33
Highest Valid: 83.46 ± 0.37
Train with the best valid: 90.68 ± 4.10
Test with the best valid epoch: 90.68 ± 4.10

============================= JOB FEEDBACK =============================

Job ID: 1441245
Cluster: haic
User/Group: cc7738/aifb
Account: aifb
State: COMPLETED (exit code 0)
Partition: normal
Nodes: 1
Cores per node: 4
Nodelist: haicn1710
CPU Utilized: 07:51:19
CPU Efficiency: 24.94% of 1-07:30:04 core-walltime
Job Wall-clock time: 07:52:31
Starttime: Sat Jun 15 15:51:12 2024
Endtime: Sat Jun 15 23:43:43 2024
Memory Utilized: 20.22 GB
Memory Efficiency: 4.13% of 489.84 GB
Energy Consumed: 16271980 Joule / 4519.99444444444 Watthours
Average node power draw: 573.947303446087 Watt
