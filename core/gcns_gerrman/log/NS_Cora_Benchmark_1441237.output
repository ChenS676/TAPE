Namespace(cfg_file='core/yamls/pubmed/gcns/ns_gnn_models.yaml', sweep_file='core/yamls/cora/gcns/gae_sp1.yaml', data='pubmed', bs=32768, device='cuda:0', epochs=200, model='GraphSage', score='mlp_score', wandb=None, repeat=3, mark_done=False, opts=[])
DDEVICE:  cuda:0 cuda:0
cuda
Number of available CUDA devices: 4
Number of available CUDA devices: 4
dropout: 0.1
emb: False
hidden_channels: 256
in_channels: None
num_layers: 3
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
layers in sage:  3
Epoch: 1, Loss: 1.2044622898101807
Epoch: 2, Loss: 1.0865542888641357
Epoch: 3, Loss: 0.8545397520065308
Epoch: 4, Loss: 0.7547084093093872
Epoch: 5, Loss: 0.653029203414917
Epoch: 6, Loss: 0.49076974391937256
Epoch: 7, Loss: 0.45319777727127075
Epoch: 8, Loss: 0.37409937381744385
Epoch: 9, Loss: 0.3412213921546936
Epoch: 10, Loss: 0.3147653341293335
Epoch: 11, Loss: 0.3331838548183441
Epoch: 12, Loss: 0.3167596757411957
Epoch: 13, Loss: 0.3208698630332947
Epoch: 14, Loss: 0.3410942554473877
Epoch: 15, Loss: 0.26469191908836365
Epoch: 16, Loss: 0.2718447744846344
Epoch: 17, Loss: 0.3065853416919708
Epoch: 18, Loss: 0.23324160277843475
Epoch: 19, Loss: 0.3156473636627197
Epoch: 20, Loss: 0.32378923892974854
Epoch: 21, Loss: 0.25784245133399963
Epoch: 22, Loss: 0.32454419136047363
Epoch: 23, Loss: 0.28411370515823364
Epoch: 24, Loss: 0.19772619009017944
Epoch: 25, Loss: 0.24454966187477112
Epoch: 26, Loss: 0.33355265855789185
Epoch: 27, Loss: 0.22864606976509094
Epoch: 28, Loss: 0.2841312885284424
Epoch: 29, Loss: 0.2355700433254242
Epoch: 30, Loss: 0.36956965923309326
Epoch: 31, Loss: 0.23414760828018188
Epoch: 32, Loss: 0.26205354928970337
Epoch: 33, Loss: 0.16387495398521423
Epoch: 34, Loss: 0.34783199429512024
Epoch: 35, Loss: 0.28965863585472107
Epoch: 36, Loss: 0.29240116477012634
Epoch: 37, Loss: 0.17737078666687012
Epoch: 38, Loss: 0.25920817255973816
Epoch: 39, Loss: 0.2828395366668701
Epoch: 40, Loss: 0.3026634156703949
Epoch: 41, Loss: 0.22640204429626465
Epoch: 42, Loss: 0.21259060502052307
Epoch: 43, Loss: 0.3150535225868225
Epoch: 44, Loss: 0.36515697836875916
Epoch: 45, Loss: 0.33010679483413696
Epoch: 46, Loss: 0.31283825635910034
Epoch: 47, Loss: 0.20798838138580322
Epoch: 48, Loss: 0.18538443744182587
Epoch: 49, Loss: 0.2667418420314789
Epoch: 50, Loss: 0.28827494382858276
Epoch: 51, Loss: 0.2528185248374939
Epoch: 52, Loss: 0.34872549772262573
Epoch: 53, Loss: 0.17266187071800232
Epoch: 54, Loss: 0.21138861775398254
Epoch: 55, Loss: 0.3283980190753937
Epoch: 56, Loss: 0.2841598689556122
Epoch: 57, Loss: 0.18145453929901123
Epoch: 58, Loss: 0.2583143711090088
Epoch: 59, Loss: 0.23984287679195404
Epoch: 60, Loss: 0.23296324908733368
Epoch: 61, Loss: 0.18099147081375122
Epoch: 62, Loss: 0.24618911743164062
Epoch: 63, Loss: 0.41110509634017944
Epoch: 64, Loss: 0.19596855342388153
Epoch: 65, Loss: 0.24377408623695374
Epoch: 66, Loss: 0.6338752508163452
Epoch: 67, Loss: 0.24971772730350494
Epoch: 68, Loss: 0.4492272138595581
Epoch: 69, Loss: 0.37226951122283936
Epoch: 70, Loss: 0.28190192580223083
Epoch: 71, Loss: 0.16461551189422607
Epoch: 72, Loss: 0.27216944098472595
Epoch: 73, Loss: 0.16223929822444916
Epoch: 74, Loss: 0.24996785819530487
Epoch: 75, Loss: 0.3782641589641571
Epoch: 76, Loss: 0.3394486904144287
Epoch: 77, Loss: 0.28582024574279785
Epoch: 78, Loss: 0.4796651601791382
Epoch: 79, Loss: 0.36780938506126404
Epoch: 80, Loss: 0.32798492908477783
Epoch: 81, Loss: 0.35693323612213135
Epoch: 82, Loss: 0.28583478927612305
Epoch: 83, Loss: 0.17523866891860962
Epoch: 84, Loss: 0.253562867641449
Epoch: 85, Loss: 0.5170336365699768
Epoch: 86, Loss: 0.2693450152873993
Epoch: 87, Loss: 0.33463817834854126
Epoch: 88, Loss: 0.3141421973705292
Epoch: 89, Loss: 0.21459662914276123
Epoch: 90, Loss: 0.2646480202674866
Epoch: 91, Loss: 0.3280259668827057
Epoch: 92, Loss: 0.14755292236804962
Epoch: 93, Loss: 0.247460275888443
Epoch: 94, Loss: 0.22862842679023743
Epoch: 95, Loss: 0.16365453600883484
Epoch: 96, Loss: 0.6649398803710938
Epoch: 97, Loss: 0.22627580165863037
Epoch: 98, Loss: 0.1849353164434433
Epoch: 99, Loss: 0.33952125906944275
Epoch: 100, Loss: 0.18286684155464172
Epoch: 101, Loss: 0.38063573837280273
Epoch: 102, Loss: 0.31226956844329834
Epoch: 103, Loss: 0.2302713394165039
Epoch: 104, Loss: 0.22673295438289642
Epoch: 105, Loss: 0.4822498559951782
Epoch: 106, Loss: 0.23051831126213074
Epoch: 107, Loss: 0.2879064977169037
Epoch: 108, Loss: 0.35369932651519775
Epoch: 109, Loss: 0.40831616520881653
Epoch: 110, Loss: 0.4980979561805725
Epoch: 111, Loss: 0.4094800353050232
Epoch: 112, Loss: 0.4988083243370056
Epoch: 113, Loss: 0.19809700548648834
Epoch: 114, Loss: 0.5644271373748779
Epoch: 115, Loss: 0.2973257005214691
Epoch: 116, Loss: 0.26080960035324097
Epoch: 117, Loss: 0.2708321213722229
Epoch: 118, Loss: 0.18133123219013214
Epoch: 119, Loss: 0.24985171854496002
Epoch: 120, Loss: 0.5414650440216064
Epoch: 121, Loss: 0.1567510962486267
Epoch: 122, Loss: 0.4052863121032715
Epoch: 123, Loss: 0.5509001016616821
Epoch: 124, Loss: 0.4484981894493103
Epoch: 125, Loss: 0.15137529373168945
Epoch: 126, Loss: 0.14825916290283203
Epoch: 127, Loss: 0.3873072564601898
Epoch: 128, Loss: 0.1907341033220291
Epoch: 129, Loss: 0.585055410861969
Epoch: 130, Loss: 0.3639829158782959
Epoch: 131, Loss: 0.2816108465194702
Epoch: 132, Loss: 0.3598957359790802
Epoch: 133, Loss: 0.47468408942222595
Epoch: 134, Loss: 0.28338849544525146
Epoch: 135, Loss: 0.3257123827934265
Epoch: 136, Loss: 0.48557254672050476
Epoch: 137, Loss: 0.37271612882614136
Epoch: 138, Loss: 0.3131977617740631
Epoch: 139, Loss: 0.16715377569198608
Epoch: 140, Loss: 0.6743327379226685
Epoch: 141, Loss: 0.20218601822853088
Epoch: 142, Loss: 0.5341248512268066
Epoch: 143, Loss: 0.3171573579311371
Epoch: 144, Loss: 0.6426304578781128
Epoch: 145, Loss: 0.2456420511007309
Epoch: 146, Loss: 0.37701916694641113
Epoch: 147, Loss: 0.4172866642475128
Epoch: 148, Loss: 0.3915643095970154
Epoch: 149, Loss: 0.4183013439178467
Epoch: 150, Loss: 0.4759211540222168
Epoch: 151, Loss: 0.7460501194000244
Epoch: 152, Loss: 0.38417136669158936
Epoch: 153, Loss: 0.24420207738876343
Epoch: 154, Loss: 0.38822031021118164
Epoch: 155, Loss: 0.38910478353500366
Epoch: 156, Loss: 0.4428909420967102
Epoch: 157, Loss: 0.3800177574157715
Epoch: 158, Loss: 0.33048349618911743
Epoch: 159, Loss: 0.31679245829582214
Epoch: 160, Loss: 0.4818130135536194
Epoch: 161, Loss: 0.8015851378440857
Epoch: 162, Loss: 0.929545521736145
Epoch: 163, Loss: 0.22733229398727417
Epoch: 164, Loss: 0.39437174797058105
Epoch: 165, Loss: 0.3094543516635895
Epoch: 166, Loss: 0.49551424384117126
Epoch: 167, Loss: 0.5427578687667847
Epoch: 168, Loss: 0.37313076853752136
Epoch: 169, Loss: 0.3847867250442505
Epoch: 170, Loss: 0.3040963411331177
Epoch: 171, Loss: 0.2664126455783844
Epoch: 172, Loss: 0.38528895378112793
Epoch: 173, Loss: 0.5652012228965759
Epoch: 174, Loss: 0.16471701860427856
Epoch: 175, Loss: 0.3235567510128021
Epoch: 176, Loss: 0.17438006401062012
Epoch: 177, Loss: 0.30948132276535034
Epoch: 178, Loss: 0.41888946294784546
Epoch: 179, Loss: 0.3807661235332489
Epoch: 180, Loss: 0.20022593438625336
Epoch: 181, Loss: 0.3598165512084961
Epoch: 182, Loss: 0.2025526762008667
Epoch: 183, Loss: 0.8136715888977051
Epoch: 184, Loss: 0.551073431968689
Epoch: 185, Loss: 0.26799511909484863
Epoch: 186, Loss: 0.1824449896812439
Epoch: 187, Loss: 0.24288105964660645
Epoch: 188, Loss: 0.46157723665237427
Epoch: 189, Loss: 0.2735106348991394
Epoch: 190, Loss: 0.2595120072364807
Epoch: 191, Loss: 0.4105908274650574
Epoch: 192, Loss: 0.3106737434864044
Epoch: 193, Loss: 0.309343546628952
Epoch: 194, Loss: 0.5985301733016968
Epoch: 195, Loss: 0.4151366651058197
Epoch: 196, Loss: 0.26074230670928955
Epoch: 197, Loss: 0.28434687852859497
Epoch: 198, Loss: 0.4052433371543884
Epoch: 199, Loss: 0.15431036055088043
Epoch: 200, Loss: 0.17499062418937683
Number of available CUDA devices: 4
Number of available CUDA devices: 4
batch_size_sampler: 128
dropout: 0.1
emb: False
hidden_channels: 16
in_channels: 500
num_hops: 6
num_layers: 3
num_neighbors: 5
out_channels: 16
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml

============================= JOB FEEDBACK =============================

Job ID: 1441237
Cluster: haic
User/Group: cc7738/aifb
Account: aifb
State: TIMEOUT (exit code 0)
Partition: normal
Nodes: 1
Cores per node: 2
Nodelist: haicn1710
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 16:00:50 core-walltime
Job Wall-clock time: 08:00:25
Starttime: Sat Jun 15 07:50:45 2024
Endtime: Sat Jun 15 15:51:10 2024
Memory Utilized: 7.92 GB
Memory Efficiency: 1.62% of 489.84 GB
Energy Consumed: 16534586 Joule / 4592.94055555556 Watthours
Average node power draw: 573.619635732871 Watt
