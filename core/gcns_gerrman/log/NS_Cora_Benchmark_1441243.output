Namespace(cfg_file='core/yamls/pubmed/gcns/ns_gnn_models.yaml', sweep_file='core/yamls/cora/gcns/gae_sp1.yaml', data='pubmed', bs=32768, device='cuda:0', epochs=150, model='VGAE', score='mlp_score', wandb=None, repeat=3, mark_done=False, opts=[])
DDEVICE:  cuda:0 cuda:0
cuda
Number of available CUDA devices: 4
Number of available CUDA devices: 4
dropout: 0
hidden_channels: 256
in_channels: None
negative_slope: 0.2
num_layers: 3
out_channels: 128
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
Epoch: 1, Loss: 1.2816065549850464
Epoch: 2, Loss: 1.2568272352218628
Epoch: 3, Loss: 1.56087064743042
Epoch: 4, Loss: 1.11564302444458
Epoch: 5, Loss: 1.3154728412628174
Epoch: 6, Loss: 0.7374964952468872
Epoch: 7, Loss: 1.0929991006851196
Epoch: 8, Loss: 1.0428179502487183
Epoch: 9, Loss: 1.155256986618042
Epoch: 10, Loss: 0.9600380659103394
Epoch: 11, Loss: 0.8803368806838989
Epoch: 12, Loss: 0.944164514541626
Epoch: 13, Loss: 0.8478844165802002
Epoch: 14, Loss: 0.8249777555465698
Epoch: 15, Loss: 0.9526951909065247
Epoch: 16, Loss: 1.1473078727722168
Epoch: 17, Loss: 1.0515906810760498
Epoch: 18, Loss: 1.0154345035552979
Epoch: 19, Loss: 0.9498388767242432
Epoch: 20, Loss: 1.1317932605743408
Epoch: 21, Loss: 0.7921404838562012
Epoch: 22, Loss: 1.281948447227478
Epoch: 23, Loss: 0.8539130687713623
Epoch: 24, Loss: 0.7814591526985168
Epoch: 25, Loss: 0.8050297498703003
Epoch: 26, Loss: 0.9085818529129028
Epoch: 27, Loss: 1.3040225505828857
Epoch: 28, Loss: 0.8644501566886902
Epoch: 29, Loss: 0.7493091225624084
Epoch: 30, Loss: 0.5749961733818054
Epoch: 31, Loss: 0.9071954488754272
Epoch: 32, Loss: 0.8862472176551819
Epoch: 33, Loss: 0.9704930782318115
Epoch: 34, Loss: 0.7772505879402161
Epoch: 35, Loss: 0.8203670382499695
Epoch: 36, Loss: 1.0907100439071655
Epoch: 37, Loss: 0.9930406808853149
Epoch: 38, Loss: 0.8073025345802307
Epoch: 39, Loss: 0.899207592010498
Epoch: 40, Loss: 0.8295716047286987
Epoch: 41, Loss: 1.0243099927902222
Epoch: 42, Loss: 0.8319856524467468
Epoch: 43, Loss: 0.8997939825057983
Epoch: 44, Loss: 1.376097559928894
Epoch: 45, Loss: 1.1006522178649902
Epoch: 46, Loss: 0.773215651512146
Epoch: 47, Loss: 0.7277830243110657
Epoch: 48, Loss: 0.6855171918869019
Epoch: 49, Loss: 0.8071302771568298
Epoch: 50, Loss: 0.8205831050872803
Epoch: 51, Loss: 0.7349637746810913
Epoch: 52, Loss: 0.6287636160850525
Epoch: 53, Loss: 1.1584844589233398
Epoch: 54, Loss: 0.9729880690574646
Epoch: 55, Loss: 0.9994368553161621
Epoch: 56, Loss: 0.8825815916061401
Epoch: 57, Loss: 0.751351535320282
Epoch: 58, Loss: 0.9936699271202087
Epoch: 59, Loss: 0.8668733835220337
Epoch: 60, Loss: 0.9211805462837219
Epoch: 61, Loss: 1.0290964841842651
Epoch: 62, Loss: 0.9395348429679871
Epoch: 63, Loss: 0.7377341389656067
Epoch: 64, Loss: 0.629096508026123
Epoch: 65, Loss: 1.1447793245315552
Epoch: 66, Loss: 0.8508471846580505
Epoch: 67, Loss: 0.8006390929222107
Epoch: 68, Loss: 0.9237976670265198
Epoch: 69, Loss: 0.9344432950019836
Epoch: 70, Loss: 0.9064391851425171
Epoch: 71, Loss: 0.9209161996841431
Epoch: 72, Loss: 0.8632543683052063
Epoch: 73, Loss: 1.2129982709884644
Epoch: 74, Loss: 0.6453922986984253
Epoch: 75, Loss: 1.0196576118469238
Epoch: 76, Loss: 0.6288712024688721
Epoch: 77, Loss: 1.1420328617095947
Epoch: 78, Loss: 0.8913095593452454
Epoch: 79, Loss: 0.7926363945007324
Epoch: 80, Loss: 1.02327299118042
Epoch: 81, Loss: 1.1346237659454346
Epoch: 82, Loss: 0.771723747253418
Epoch: 83, Loss: 0.6282433271408081
Epoch: 84, Loss: 0.6415441036224365
Epoch: 85, Loss: 1.0105397701263428
Epoch: 86, Loss: 1.0345796346664429
Epoch: 87, Loss: 0.8855193257331848
Epoch: 88, Loss: 1.1752490997314453
Epoch: 89, Loss: 0.7488186359405518
Epoch: 90, Loss: 0.830710768699646
Epoch: 91, Loss: 1.1801977157592773
Epoch: 92, Loss: 1.012427568435669
Epoch: 93, Loss: 0.8074666261672974
Epoch: 94, Loss: 0.6287011504173279
Epoch: 95, Loss: 1.0545308589935303
Epoch: 96, Loss: 0.773402750492096
Epoch: 97, Loss: 0.7048671841621399
Epoch: 98, Loss: 0.8512024879455566
Epoch: 99, Loss: 1.526757836341858
Epoch: 100, Loss: 0.9779187440872192
Epoch: 101, Loss: 0.6972323060035706
Epoch: 102, Loss: 1.0072253942489624
Epoch: 103, Loss: 0.8242745399475098
Epoch: 104, Loss: 0.5591500997543335
Epoch: 105, Loss: 1.1431891918182373
Epoch: 106, Loss: 0.8072119951248169
Epoch: 107, Loss: 0.575072169303894
Epoch: 108, Loss: 1.1689488887786865
Epoch: 109, Loss: 0.8392797708511353
Epoch: 110, Loss: 0.783073365688324
Epoch: 111, Loss: 0.6486870646476746
Epoch: 112, Loss: 1.0009251832962036
Epoch: 113, Loss: 1.0382047891616821
Epoch: 114, Loss: 0.82676762342453
Epoch: 115, Loss: 1.0055086612701416
Epoch: 116, Loss: 1.3272544145584106
Epoch: 117, Loss: 0.9586065411567688
Epoch: 118, Loss: 0.8140128254890442
Epoch: 119, Loss: 0.6729978322982788
Epoch: 120, Loss: 0.6404654383659363
Epoch: 121, Loss: 0.724351704120636
Epoch: 122, Loss: 1.0421658754348755
Epoch: 123, Loss: 1.0982881784439087
Epoch: 124, Loss: 0.9888777732849121
Epoch: 125, Loss: 0.48387107253074646
Epoch: 126, Loss: 0.4101557433605194
Epoch: 127, Loss: 0.5926816463470459
Epoch: 128, Loss: 0.8593235015869141
Epoch: 129, Loss: 0.3354324698448181
Epoch: 130, Loss: 0.5461829304695129
Epoch: 131, Loss: 1.053282618522644
Epoch: 132, Loss: 1.190760612487793
Epoch: 133, Loss: 0.9316748380661011
Epoch: 134, Loss: 0.7255057096481323
Epoch: 135, Loss: 1.3616981506347656
Epoch: 136, Loss: 1.335105061531067
Epoch: 137, Loss: 0.8941059708595276
Epoch: 138, Loss: 0.7253275513648987
Epoch: 139, Loss: 0.5262972712516785
Epoch: 140, Loss: 1.07441246509552
Epoch: 141, Loss: 1.059597134590149
Epoch: 142, Loss: 0.9984849095344543
Epoch: 143, Loss: 0.5600501298904419
Epoch: 144, Loss: 0.69547438621521
Epoch: 145, Loss: 0.9926799535751343
Epoch: 146, Loss: 1.1610335111618042
Epoch: 147, Loss: 1.2211151123046875
Epoch: 148, Loss: 0.5151865482330322
Epoch: 149, Loss: 1.310983657836914
Epoch: 150, Loss: 0.824881911277771
Number of available CUDA devices: 4
Number of available CUDA devices: 4
batch_size_sampler: 128
dropout: 0
hidden_channels: 32
in_channels: 500
negative_slope: 0.2
num_hops: 5
num_layers: 3
num_neighbors: 5
out_channels: 32
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
Epoch: 1, Loss: 1.3492058515548706
Epoch: 2, Loss: 1.2091292142868042
Epoch: 3, Loss: 1.253867268562317
Epoch: 4, Loss: 1.0424015522003174
Epoch: 5, Loss: 1.2191498279571533
Epoch: 6, Loss: 1.1775261163711548
Epoch: 7, Loss: 1.044786810874939
Epoch: 8, Loss: 1.4272013902664185
Epoch: 9, Loss: 1.1463221311569214
Epoch: 10, Loss: 1.111655592918396
Epoch: 11, Loss: 1.032456398010254
Epoch: 12, Loss: 1.00300133228302
Epoch: 13, Loss: 0.9047264456748962
Epoch: 14, Loss: 1.0439237356185913
Epoch: 15, Loss: 0.9758980870246887
Epoch: 16, Loss: 1.3654447793960571
Epoch: 17, Loss: 1.3489346504211426
Epoch: 18, Loss: 0.9527376890182495
Epoch: 19, Loss: 1.024598479270935
Epoch: 20, Loss: 0.9234703183174133
Epoch: 21, Loss: 1.0945185422897339
Epoch: 22, Loss: 1.33954656124115
Epoch: 23, Loss: 0.8188942670822144
Epoch: 24, Loss: 0.8675668239593506
Epoch: 25, Loss: 1.16764497756958
Epoch: 26, Loss: 0.8921866416931152
Epoch: 27, Loss: 0.8819906115531921
Epoch: 28, Loss: 0.9269270896911621
Epoch: 29, Loss: 0.9731642007827759
Epoch: 30, Loss: 1.0209932327270508
Epoch: 31, Loss: 1.124566674232483
Epoch: 32, Loss: 0.6614611744880676
Epoch: 33, Loss: 1.0568491220474243
Epoch: 34, Loss: 1.1369444131851196
Epoch: 35, Loss: 0.6100192070007324
Epoch: 36, Loss: 0.8425955772399902
Epoch: 37, Loss: 0.48721379041671753
Epoch: 38, Loss: 0.978722870349884
Epoch: 39, Loss: 0.5903126001358032
Epoch: 40, Loss: 0.8193488121032715
Epoch: 41, Loss: 0.8195942044258118
Epoch: 42, Loss: 0.7649165391921997
Epoch: 43, Loss: 1.1644269227981567
Epoch: 44, Loss: 1.0406584739685059
Epoch: 45, Loss: 0.9299144148826599
Epoch: 46, Loss: 1.1685494184494019
Epoch: 47, Loss: 1.1714744567871094
Epoch: 48, Loss: 0.9617637395858765
Epoch: 49, Loss: 0.8813762068748474
Epoch: 50, Loss: 0.8359760046005249
Epoch: 51, Loss: 0.9475343823432922
Epoch: 52, Loss: 0.8827250599861145
Epoch: 53, Loss: 1.35630464553833
Epoch: 54, Loss: 1.0161218643188477
Epoch: 55, Loss: 1.122438907623291
Epoch: 56, Loss: 1.0120744705200195
Epoch: 57, Loss: 1.0326131582260132
Epoch: 58, Loss: 0.8032897710800171
Epoch: 59, Loss: 0.8151324391365051
Epoch: 60, Loss: 1.1157938241958618
Epoch: 61, Loss: 1.1653261184692383
Epoch: 62, Loss: 0.5235174894332886
Epoch: 63, Loss: 0.7313984632492065
Epoch: 64, Loss: 0.6506967544555664
Epoch: 65, Loss: 0.604023277759552
Epoch: 66, Loss: 0.9795518517494202
Epoch: 67, Loss: 0.7312886714935303
Epoch: 68, Loss: 0.8451192378997803
Epoch: 69, Loss: 0.9755281805992126
Epoch: 70, Loss: 1.0170930624008179
Epoch: 71, Loss: 0.8057030439376831
Epoch: 72, Loss: 1.1236644983291626
Epoch: 73, Loss: 0.9823750257492065
Epoch: 74, Loss: 0.9819415807723999
Epoch: 75, Loss: 1.0548062324523926
Epoch: 76, Loss: 0.522244393825531
Epoch: 77, Loss: 1.2359309196472168
Epoch: 78, Loss: 0.9634625315666199
Epoch: 79, Loss: 0.6588190197944641
Epoch: 80, Loss: 0.7192475199699402
Epoch: 81, Loss: 0.6867774128913879
Epoch: 82, Loss: 0.7780344486236572
Epoch: 83, Loss: 0.6658008694648743
Epoch: 84, Loss: 1.0615675449371338
Epoch: 85, Loss: 0.801612377166748
Epoch: 86, Loss: 1.0731176137924194
Epoch: 87, Loss: 0.9466069936752319
Epoch: 88, Loss: 0.9022160768508911
Epoch: 89, Loss: 0.8289049863815308
Epoch: 90, Loss: 0.8955153226852417
Epoch: 91, Loss: 1.0568338632583618
Epoch: 92, Loss: 0.6703830361366272
Epoch: 93, Loss: 0.7294041514396667
Epoch: 94, Loss: 0.8216203451156616
Epoch: 95, Loss: 0.5579918026924133
Epoch: 96, Loss: 0.9776308536529541
Epoch: 97, Loss: 1.0241950750350952
Epoch: 98, Loss: 1.241026520729065
Epoch: 99, Loss: 0.9797494411468506
Epoch: 100, Loss: 0.6693235635757446
Epoch: 101, Loss: 0.9520337581634521
Epoch: 102, Loss: 1.1137702465057373
Epoch: 103, Loss: 0.8683740496635437
Epoch: 104, Loss: 1.001151204109192
Epoch: 105, Loss: 0.8765730857849121
Epoch: 106, Loss: 0.6698161959648132
Epoch: 107, Loss: 0.504935622215271
Epoch: 108, Loss: 0.7769545316696167
Epoch: 109, Loss: 0.7252912521362305
Epoch: 110, Loss: 1.0082212686538696
Epoch: 111, Loss: 1.2214640378952026
Epoch: 112, Loss: 0.9207065105438232
Epoch: 113, Loss: 1.0176584720611572
Epoch: 114, Loss: 1.0809458494186401
Epoch: 115, Loss: 0.948584258556366
Epoch: 116, Loss: 0.827299952507019
Epoch: 117, Loss: 1.031532883644104
Epoch: 118, Loss: 1.0788685083389282
Epoch: 119, Loss: 0.7605180740356445
Epoch: 120, Loss: 0.46889159083366394
Epoch: 121, Loss: 0.7597804665565491
Epoch: 122, Loss: 0.6886045336723328
Epoch: 123, Loss: 0.6935189366340637
Epoch: 124, Loss: 1.0955450534820557
Epoch: 125, Loss: 0.675229549407959
Epoch: 126, Loss: 0.6612318754196167
Epoch: 127, Loss: 0.9279100894927979
Epoch: 128, Loss: 0.6688144207000732
Epoch: 129, Loss: 0.5790414810180664
Epoch: 130, Loss: 0.7222930788993835
Epoch: 131, Loss: 1.0946284532546997
Epoch: 132, Loss: 0.655899167060852
Epoch: 133, Loss: 0.7950157523155212
Epoch: 134, Loss: 1.0011790990829468
Epoch: 135, Loss: 0.6351521015167236
Epoch: 136, Loss: 1.4812949895858765
Epoch: 137, Loss: 0.8200084567070007
Epoch: 138, Loss: 0.890415608882904
Epoch: 139, Loss: 1.267595648765564
Epoch: 140, Loss: 0.8527202010154724
Epoch: 141, Loss: 1.1006189584732056
Epoch: 142, Loss: 1.0259422063827515
Epoch: 143, Loss: 1.023683786392212
Epoch: 144, Loss: 0.7109397053718567
Epoch: 145, Loss: 0.6466512680053711
Epoch: 146, Loss: 0.7572506070137024
Epoch: 147, Loss: 0.6996785402297974
Epoch: 148, Loss: 0.5705792903900146
Epoch: 149, Loss: 0.608894944190979
Epoch: 150, Loss: 0.887140691280365
Number of available CUDA devices: 4
Number of available CUDA devices: 4
batch_size_sampler: 128
dropout: 0
hidden_channels: 32
in_channels: 500
negative_slope: 0.2
num_hops: 5
num_layers: 3
num_neighbors: 5
out_channels: 32
original config file saved to results/ns_gnn_models-Tune/ns_gnn_models.yaml
Epoch: 1, Loss: 1.2999534606933594
Epoch: 2, Loss: 1.2332978248596191
Epoch: 3, Loss: 1.132871150970459
Epoch: 4, Loss: 1.0840251445770264
Epoch: 5, Loss: 1.1077684164047241
Epoch: 6, Loss: 1.2150654792785645
Epoch: 7, Loss: 1.130813717842102
Epoch: 8, Loss: 0.9894657731056213
Epoch: 9, Loss: 1.0443555116653442
Epoch: 10, Loss: 0.5923386812210083
Epoch: 11, Loss: 1.0950109958648682
Epoch: 12, Loss: 1.0109444856643677
Epoch: 13, Loss: 1.265212893486023
Epoch: 14, Loss: 0.9648862481117249
Epoch: 15, Loss: 0.8243338465690613
Epoch: 16, Loss: 1.1156740188598633
Epoch: 17, Loss: 0.7208108305931091
Epoch: 18, Loss: 0.9766069650650024
Epoch: 19, Loss: 1.2173974514007568
Epoch: 20, Loss: 0.9879190921783447
Epoch: 21, Loss: 0.7524383068084717
Epoch: 22, Loss: 0.8951184749603271
Epoch: 23, Loss: 0.8393241167068481
Epoch: 24, Loss: 0.7041724920272827
Epoch: 25, Loss: 0.721150815486908
Epoch: 26, Loss: 0.8718112707138062
Epoch: 27, Loss: 0.8118420243263245
Epoch: 28, Loss: 1.206702709197998
Epoch: 29, Loss: 0.7851448059082031
Epoch: 30, Loss: 0.7968190312385559
Epoch: 31, Loss: 0.8711728453636169
Epoch: 32, Loss: 0.7357136011123657
Epoch: 33, Loss: 0.9256110787391663
Epoch: 34, Loss: 0.9514251351356506
Epoch: 35, Loss: 0.8426758646965027
Epoch: 36, Loss: 1.002331018447876
Epoch: 37, Loss: 1.0060862302780151
Epoch: 38, Loss: 0.7452265620231628
Epoch: 39, Loss: 0.8948553204536438
Epoch: 40, Loss: 0.7878392934799194
Epoch: 41, Loss: 0.6780090928077698
Epoch: 42, Loss: 0.9794093370437622
Epoch: 43, Loss: 0.8475704193115234
Epoch: 44, Loss: 0.9389252662658691
Epoch: 45, Loss: 1.2379239797592163
Epoch: 46, Loss: 0.9934353828430176
Epoch: 47, Loss: 1.1485470533370972
Epoch: 48, Loss: 0.8703685998916626
Epoch: 49, Loss: 0.44909632205963135
Epoch: 50, Loss: 0.9473217725753784
Epoch: 51, Loss: 0.9447838068008423
Epoch: 52, Loss: 0.7752882242202759
Epoch: 53, Loss: 0.46114233136177063
Epoch: 54, Loss: 1.0865346193313599
Epoch: 55, Loss: 0.8449097871780396
Epoch: 56, Loss: 0.7807884216308594
Epoch: 57, Loss: 1.0434985160827637
Epoch: 58, Loss: 0.9031976461410522
Epoch: 59, Loss: 1.0288132429122925
Epoch: 60, Loss: 0.8161711692810059
Epoch: 61, Loss: 0.8141824007034302
Epoch: 62, Loss: 1.376262903213501
Epoch: 63, Loss: 0.8479136228561401
Epoch: 64, Loss: 0.7839536666870117
Epoch: 65, Loss: 0.906101405620575
Epoch: 66, Loss: 0.756112277507782
Epoch: 67, Loss: 0.863709032535553
Epoch: 68, Loss: 0.8131840825080872
Epoch: 69, Loss: 1.0606008768081665
Epoch: 70, Loss: 0.7767574191093445
Epoch: 71, Loss: 0.9566942453384399
Epoch: 72, Loss: 0.7674475312232971
Epoch: 73, Loss: 0.849825382232666
Epoch: 74, Loss: 0.8780245780944824
Epoch: 75, Loss: 0.8524004817008972
Epoch: 76, Loss: 0.7963943481445312
Epoch: 77, Loss: 0.43191370368003845
Epoch: 78, Loss: 1.1352770328521729
Epoch: 79, Loss: 1.167249321937561
Epoch: 80, Loss: 0.6565952301025391
Epoch: 81, Loss: 1.0569261312484741
Epoch: 82, Loss: 0.7244422435760498
Epoch: 83, Loss: 0.9112030267715454
Epoch: 84, Loss: 0.873127281665802
Epoch: 85, Loss: 0.7052909135818481
Epoch: 86, Loss: 0.805107593536377
Epoch: 87, Loss: 0.7824697494506836
Epoch: 88, Loss: 0.6528664827346802
Epoch: 89, Loss: 0.7009010314941406
Epoch: 90, Loss: 0.5619140863418579
Epoch: 91, Loss: 0.49652108550071716
Epoch: 92, Loss: 0.8143873810768127
Epoch: 93, Loss: 0.7184357047080994
Epoch: 94, Loss: 0.6756314635276794
Epoch: 95, Loss: 1.0423407554626465
Epoch: 96, Loss: 0.7739725708961487
Epoch: 97, Loss: 0.6622015833854675
Epoch: 98, Loss: 0.8491660356521606
Epoch: 99, Loss: 0.4684540033340454
Epoch: 100, Loss: 0.6930563449859619
Epoch: 101, Loss: 0.6102443337440491
Epoch: 102, Loss: 0.6842568516731262
Epoch: 103, Loss: 0.7103838920593262
Epoch: 104, Loss: 0.6983667016029358
Epoch: 105, Loss: 1.7387316226959229
Epoch: 106, Loss: 0.9043368697166443
Epoch: 107, Loss: 0.9054468274116516
Epoch: 108, Loss: 0.7231305837631226
Epoch: 109, Loss: 1.2035218477249146
Epoch: 110, Loss: 1.5116422176361084
Epoch: 111, Loss: 0.5647540092468262
Epoch: 112, Loss: 0.8666214942932129
Epoch: 113, Loss: 0.867118239402771
Epoch: 114, Loss: 1.2102631330490112
Epoch: 115, Loss: 0.815656840801239
Epoch: 116, Loss: 0.7985860109329224
Epoch: 117, Loss: 0.7642951011657715
Epoch: 118, Loss: 0.7155572175979614
Epoch: 119, Loss: 0.5848535299301147
Epoch: 120, Loss: 0.8586215376853943
Epoch: 121, Loss: 0.8202563524246216
Epoch: 122, Loss: 0.8912774324417114
Epoch: 123, Loss: 0.9630448818206787
Epoch: 124, Loss: 1.131943702697754
Epoch: 125, Loss: 0.6091883182525635
Epoch: 126, Loss: 1.0401724576950073
Epoch: 127, Loss: 0.7056187987327576
Epoch: 128, Loss: 0.9067595601081848
Epoch: 129, Loss: 0.7423336505889893
Epoch: 130, Loss: 0.7200273275375366
Epoch: 131, Loss: 0.757474422454834
Epoch: 132, Loss: 0.5746681094169617
Epoch: 133, Loss: 0.9592658877372742
Epoch: 134, Loss: 0.8662769794464111
Epoch: 135, Loss: 0.750062108039856
Epoch: 136, Loss: 0.594468891620636
Epoch: 137, Loss: 0.9140071272850037
Epoch: 138, Loss: 1.1013202667236328
Epoch: 139, Loss: 0.6697941422462463
Epoch: 140, Loss: 0.6300384998321533
Epoch: 141, Loss: 0.7397187948226929
Epoch: 142, Loss: 0.8643860816955566
Epoch: 143, Loss: 0.6396914124488831
Epoch: 144, Loss: 1.3162964582443237
Epoch: 145, Loss: 1.0711091756820679
Epoch: 146, Loss: 0.9397408962249756
Epoch: 147, Loss: 1.058530330657959
Epoch: 148, Loss: 1.179680585861206
Epoch: 149, Loss: 0.5707587003707886
Epoch: 150, Loss: 0.6010276079177856
Hits@1
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 29.21 ± 6.81
Highest Valid: 13.28 ± 2.17
Train with the best valid: 11.13 ± 6.82
Test with the best valid epoch: 11.13 ± 6.82
Hits@3
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 40.46 ± 6.20
Highest Valid: 20.11 ± 1.37
Train with the best valid: 22.32 ± 7.19
Test with the best valid epoch: 22.32 ± 7.19
Hits@10
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 59.58 ± 1.16
Highest Valid: 33.37 ± 5.45
Train with the best valid: 34.14 ± 10.03
Test with the best valid epoch: 34.14 ± 10.03
Hits@20
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 69.45 ± 1.64
Highest Valid: 47.43 ± 5.12
Train with the best valid: 50.45 ± 14.13
Test with the best valid epoch: 50.45 ± 14.13
Hits@50
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 82.35 ± 0.72
Highest Valid: 66.44 ± 2.90
Train with the best valid: 67.89 ± 9.90
Test with the best valid epoch: 67.89 ± 9.90
Hits@100
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 89.88 ± 0.39
Highest Valid: 79.17 ± 3.35
Train with the best valid: 77.73 ± 10.44
Test with the best valid epoch: 77.73 ± 10.44
MRR
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 37.37 ± 6.00
Highest Valid: 18.51 ± 2.58
Train with the best valid: 16.75 ± 4.51
Test with the best valid epoch: 16.75 ± 4.51
mrr_hit1
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 29.21 ± 6.81
Highest Valid: 13.28 ± 2.17
Train with the best valid: 11.13 ± 6.82
Test with the best valid epoch: 11.13 ± 6.82
mrr_hit3
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 40.46 ± 6.20
Highest Valid: 20.11 ± 1.37
Train with the best valid: 22.32 ± 7.19
Test with the best valid epoch: 22.32 ± 7.19
mrr_hit10
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 59.58 ± 1.16
Highest Valid: 33.37 ± 5.45
Train with the best valid: 34.14 ± 10.03
Test with the best valid epoch: 34.14 ± 10.03
mrr_hit20
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 69.45 ± 1.64
Highest Valid: 47.43 ± 5.12
Train with the best valid: 50.45 ± 14.13
Test with the best valid epoch: 50.45 ± 14.13
mrr_hit50
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 82.35 ± 0.72
Highest Valid: 66.44 ± 2.90
Train with the best valid: 67.89 ± 9.90
Test with the best valid epoch: 67.89 ± 9.90
mrr_hit100
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 89.88 ± 0.39
Highest Valid: 79.17 ± 3.35
Train with the best valid: 77.73 ± 10.44
Test with the best valid epoch: 77.73 ± 10.44
AUC
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 99.16 ± 0.05
Highest Valid: 95.71 ± 0.37
Train with the best valid: 97.70 ± 0.10
Test with the best valid epoch: 97.70 ± 0.10
AP
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 99.86 ± 0.01
Highest Valid: 98.45 ± 0.19
Train with the best valid: 99.59 ± 0.02
Test with the best valid epoch: 99.59 ± 0.02
ACC
torch.Size([30, 3])
torch.Size([30, 3])
torch.Size([30, 3])
Highest Train: 96.06 ± 0.12
Highest Valid: 89.49 ± 0.33
Train with the best valid: 94.33 ± 0.44
Test with the best valid epoch: 94.33 ± 0.44

============================= JOB FEEDBACK =============================

Job ID: 1441243
Cluster: haic
User/Group: cc7738/aifb
Account: aifb
State: COMPLETED (exit code 0)
Partition: normal
Nodes: 1
Cores per node: 4
Nodelist: haicn1711
CPU Utilized: 07:57:50
CPU Efficiency: 24.94% of 1-07:55:56 core-walltime
Job Wall-clock time: 07:58:59
Starttime: Sat Jun 15 10:36:11 2024
Endtime: Sat Jun 15 18:35:10 2024
Memory Utilized: 4.51 GB
Memory Efficiency: 0.92% of 489.84 GB
Energy Consumed: 16577508 Joule / 4604.86333333333 Watthours
Average node power draw: 576.829673962212 Watt
