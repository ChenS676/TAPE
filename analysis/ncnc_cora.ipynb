{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:29:53.145871Z",
     "start_time": "2024-08-01T18:29:53.009990Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "import torch \n",
    "from visual import find_opt_thres, get_metric_invariant, load_results\n",
    "from matplotlib import pyplot as plt\n",
    "from visual import load_csv\n",
    "# evaluator = Evaluator(name='ogbl-collab')\n",
    "evaluator_hit = Evaluator(name='ogbl-collab')\n",
    "evaluator_mrr = Evaluator(name='ogbl-citation2')"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'find_opt_thres' from 'visual' (/Users/ruiruiwang/KIT/project/TAPE/analysis/visual.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mogb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinkproppred\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PygLinkPropPredDataset, Evaluator\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mvisual\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m find_opt_thres, get_metric_invariant, load_results\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pyplot \u001B[38;5;28;01mas\u001B[39;00m plt\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mvisual\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_csv\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'find_opt_thres' from 'visual' (/Users/ruiruiwang/KIT/project/TAPE/analysis/visual.py)"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:58:47.608646Z",
     "start_time": "2024-08-01T17:58:47.608518Z"
    }
   },
   "source": [
    "# Example usage\n",
    "FILE_PATH = '/hkfs/work/workspace/scratch/cc7738-benchmark_tag/educational_demo/'\n",
    "file_path = FILE_PATH + 'err_ncnc_llama/ncnc-cora_AUC_0.9669_MRR_0.5275.csv'\n",
    "P1, P2, pos_index, neg_index = load_results(file_path)\n",
    "best_thres_llama, best_acc_llama, pos_pred_llama, neg_pred_llama = find_opt_thres(P1, P2)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot distributions of probabilities\n",
    "plt.hist(P2, bins=100, alpha=0.5, color='blue', label='Neg Class')\n",
    "plt.hist(P1, bins=100, alpha=0.5, color='red', label='Pos Class')\n",
    "best_thres, best_acc, pos_pred, neg_pred = find_opt_thres(P1, P2)\n",
    "\n",
    "plt.axvline(best_thres, color='green', linestyle='--', label=f'Optimal Threshold = {best_thres:.2f}')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Probability Distributions with Optimal Threshold')\n",
    "plt.savefig('optimal_threshold.png')\n",
    "print(f'best_accuracy: {best_acc}, best_threshold: {best_thres}')\n",
    "\n",
    "k_list  = [0.1, 0.2, 0.3, 0.5, 1]\n",
    "pos_index = torch.tensor(pos_index)\n",
    "neg_index = torch.tensor(neg_index)\n",
    "P1 = torch.tensor(P1)\n",
    "P2 = torch.tensor(P2)\n",
    "mrr_pos2neg, mrr_neg2pos, result_auc_test, pos_edge_index_err, pos_rank_err, neg_edge_index_err, neg_rank_err = get_metric_invariant(P1, pos_index, P2, neg_index, k_list)\n",
    "\n",
    "print(mrr_pos2neg)\n",
    "print(result_auc_test)\n",
    "print(mrr_neg2pos)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming your target directory is one level up from the current working directory\n",
    "notebook_dir = os.getcwd()  \n",
    "target_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "sys.path.insert(0, target_dir)\n",
    "from core.data_utils.load import load_data_lp\n",
    "from core.graphgps.utility.utils import init_cfg_test\n",
    "\n",
    "cfg = init_cfg_test()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "splits, text, data = load_data_lp[cfg.data.name](cfg.data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "splits"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'index predicted to be neg when pos : {pos_edge_index_err.shape}')\n",
    "print(f'index predicted to be pos when neg : {neg_edge_index_err.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Pos is predicted as Neg? Type II."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in pos_edge_index_err:\n",
    "    print(f'source: {text[row[0]]}, \\n target: {text[row[1]]}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "for row in pos_edge_index_err:\n",
    "    src = text[row[0]]\n",
    "    tgt = text[row[1]]\n",
    "    display(Markdown(f\"**Source:** {src}  \\n**Target:** {tgt}\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for Error I\n",
    "https://24timezones.com/difference/frankfurt_ger/new_delhi?dt=2024-07-31T830pm \n",
    "1. Such citation indeed exists, however, it appears in a historical section.\n",
    "| ![Example Image](intro.png) |![Example Image](ref.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for Error II \n",
    "1. textual features are quite noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target:**\n",
    "\n",
    "**Title:** Cholinergic Suppression of Transmission May Allow Combined Associative Memory Function and Self-Organization in the Neocortex\n",
    "\n",
    "**Abstract:** Selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self-organization of feedforward synapses. Experimental data demonstrates cholinergic suppression of synaptic transmission in layer I (feedback synapses), and a lack of suppression in layer IV (feed-forward synapses). A network with this feature uses local rules to learn mappings which are not linearly separable. During learning, sensory stimuli and desired responses are simultaneously presented as input. Feedforward connections form self-organized representations of input, while suppressed feedback connections learn the transpose of feedforward connectivity. During recall, suppression is removed, sensory input activates the self-organized representation, and activity generates the learned response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No idea what the hell is this? Title is the Figure description and abstract is followed by the reference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source:**\n",
    "\n",
    "**Title:** Figure 8: Time Complexity of Unit Parallelism Measured on MANNA Theoretical Prediction #nodes N Time\n",
    "\n",
    "**Abstract:** Our experience showed us that flexibility in expressing a parallel algorithm for simulating neural networks is desirable even if it is not possible then to obtain the most efficient solution for any single training algorithm. We believe that the advantages of a clear and easy-to-understand program predominate the disadvantages of approaches allowing only for a specific machine or neural network algorithm. We currently investigate if other neural network models are worthwhile being parallelized, and how the resulting parallel algorithms can be composed of a few common basic building blocks and the logarithmic tree as an efficient communication structure.\n",
    "\n",
    "**References:**\n",
    "1. D. Ackley, G. Hinton, T. Sejnowski: *A Learning Algorithm for Boltzmann Machines*, Cognitive Science 9, pp. 147-169, 1985\n",
    "2. B. M. Forrest et al.: *Implementing Neural Network Models on Parallel Computers*, The Computer Journal, vol. 30, no. 5, 1987\n",
    "3. W. Giloi: *Latency Hiding in Message Passing Architectures*, International Parallel Processing Symposium, April 1994, Cancun, Mexico, IEEE Computer Society Press\n",
    "4. T. Nordstr√∂m, B. Svensson: *Using And Designing Massively Parallel Computers for Artificial Neural Networks*, Journal Of Parallel And Distributed Computing, vol. 14, pp. 260-285, 1992\n",
    "5. A. Kramer, A. Vincentelli: *Efficient Parallel Learning Algorithms for Neural Networks*, Advances in Neural Information Processing Systems I, D. Touretzky (ed.), pp. 40-48, 1989\n",
    "6. T. Kohonen: *Self-Organization and Associative Memory*, Springer-Verlag, Berlin, 1988\n",
    "7. D. A. Pomerleau, G. L. Gusciora, D. L. Touretzky, H. T. Kung: *Neural Network Simulation at Warp Speed: How We Got 17 Million Connections Per Second*, IEEE Intern. Conf. Neural Networks, July 1988\n",
    "8. A. R√ºbel: *Dynamic Selection of Training Patterns for Neural Networks: A New Method to Control the Generalization*, Technical Report 92-39, Technical University of Berlin, 1993\n",
    "9. D. E. Rumelhart, D. E. Hinton, R. J. Williams: *Learning Internal Representations by Error Propagation*, Rumelhart & McClelland (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. I, pp. 318-362, Bradford Books/MIT Press, Cambridge, MA, 1986\n",
    "10. W. Schiffmann, M. Joost, R. Werner: *Comparison of Optimized Backpropagation Algorithms*, Proc. of the European Symposium on Artificial Neural Networks, ESANN '93, Brussels, pp. 97-104, 1993\n",
    "11. J. Schmidhuber: *Accelerated Learning in BackPropagation Nets*, Connectionism in Perspective, Elsevier Science Publishers B.V. (North-Holland), pp 439-445, 1989\n",
    "12. M. Taylor, P. Lisboa (eds.): *Techniques and Applications of Neural Networks*, Ellis Horwood, 1993\n",
    "13. M. Witbrock, M. Zagha: *An Implementation of Backpropagation Learning on GF11, a Large SIMD Parallel Computer*, Parallel Computing, vol. 14, pp. 329-346, 1990\n",
    "14. X. Zhang, M. McKenna, J. P. Mesirov, D. L. Waltz: *The Backpropagation Algorithm on Grid and Hypercube Architectures*, Parallel Computing, vol. 14, pp. 317-327, 1990"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "for row in neg_edge_index_err:\n",
    "    src = text[row[0]]\n",
    "    tgt = text[row[1]]\n",
    "    display(Markdown(f\"**Source:** {src}  \\n**Target:** {tgt}\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def adamic_adar(adj, node1, node2):\n",
    "    \"\"\"ËÆ°ÁÆóAdamic-AdarÊåáÊï∞\"\"\"\n",
    "    neighbors1 = adj[node1].nonzero(as_tuple=False).flatten()\n",
    "    neighbors2 = adj[node2].nonzero(as_tuple=False).flatten()\n",
    "    common_neighbors = torch.tensor(list(set(neighbors1.tolist()).intersection(set(neighbors2.tolist()))))\n",
    "    index = adj.sum(dim=0).squeeze()\n",
    "    aa_score = sum(1.0 / torch.log1p(index[neighbor].float()) for neighbor in common_neighbors)\n",
    "    return aa_score\n",
    "\n",
    "def resource_allocation(adj, node1, node2):\n",
    "    \"\"\"ËÆ°ÁÆóËµÑÊ∫êÂàÜÈÖçÊåáÊï∞\"\"\"\n",
    "    neighbors1 = adj[node1].nonzero(as_tuple=False).flatten()\n",
    "    neighbors2 = adj[node2].nonzero(as_tuple=False).flatten()\n",
    "    common_neighbors = torch.tensor(list(set(neighbors1.tolist()).intersection(set(neighbors2.tolist()))))\n",
    "    index = adj.sum(dim=0).squeeze()\n",
    "    ra_score = sum(1.0 / index[neighbor].float() for neighbor in common_neighbors)\n",
    "    return ra_score\n",
    "\n",
    "def pagerank(adj, node, alpha=0.85, max_iter=100):\n",
    "    \"\"\"ËÆ°ÁÆóPageRankÂæóÂàÜ\"\"\"\n",
    "    n = adj.size(0)\n",
    "    ranks = torch.ones(n, dtype=torch.float32) / n\n",
    "    for _ in range(max_iter):\n",
    "        new_ranks = (1 - alpha) / n + alpha * adj @ ranks\n",
    "        if torch.allclose(new_ranks, ranks, atol=1e-6):\n",
    "            break\n",
    "        ranks = new_ranks\n",
    "    return ranks[node]\n",
    "\n",
    "def katz(adj, node1, node2, alpha=0.005, beta=0.005, max_iter=1000):\n",
    "    \"\"\"ËÆ°ÁÆóKatzÂæóÂàÜ\"\"\"\n",
    "    n = adj.size(0)\n",
    "    I = torch.eye(n)\n",
    "    katz_matrix = alpha * I + beta * adj\n",
    "    katz_scores = torch.linalg.pinv(katz_matrix) @ torch.ones(n, dtype=torch.float32)\n",
    "\n",
    "    return katz_scores[node1] + katz_scores[node2]\n",
    "\n",
    "def shortest_path_length(adj, node1, node2):\n",
    "    \"\"\"ËÆ°ÁÆóÊúÄÁü≠Ë∑ØÂæÑÈïøÂ∫¶\"\"\"\n",
    "    n = adj.size(0)\n",
    "    dist = torch.full((n, n), float('inf'))\n",
    "    dist[torch.arange(n), torch.arange(n)] = 0\n",
    "    for i in range(adj.size(0)):\n",
    "        for j in range(adj.size(0)):\n",
    "            if adj[i, j] != 0:\n",
    "                dist[i, j] = 1\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if dist[i, j] > dist[i, k] + dist[k, j]:\n",
    "                    dist[i, j] = dist[i, k] + dist[k, j]\n",
    "    return dist[node1, node2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch_geometric import utils\n",
    "from data_utils.lcc import use_lcc\n",
    "\n",
    "splits, text, data = load_data_lp[cfg.data.name](cfg.data)\n",
    "# new_data, lcc_index, G = use_lcc(data)\n",
    "G = utils.to_dense_adj(data.edge_index).squeeze()\n",
    "G"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pos_edge_index_err.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in pos_edge_index_err:\n",
    "    src = row[0]\n",
    "    tgt = row[1]\n",
    "    try:\n",
    "        print(\"Adamic-Adar:\", adamic_adar(G, src, tgt))\n",
    "        print(\"Resource Allocation:\", resource_allocation(G, src, tgt))\n",
    "        print(\"PageRank:\", pagerank(G, src))\n",
    "        print(\"Katz:\", katz(G, src, tgt))\n",
    "        print(\"Shortest Path Length:\", shortest_path_length(G, src, tgt))\n",
    "    except:\n",
    "        print(\"Error\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in neg_edge_index_err:\n",
    "    src = row[0]\n",
    "    tgt = row[1]\n",
    "    print(\"Adamic-Adar:\", adamic_adar(G, src, tgt))\n",
    "    print(\"Resource Allocation:\", resource_allocation(G, src, tgt))\n",
    "    print(\"PageRank:\", pagerank(G, src))\n",
    "    print(\"PageRank:\", pagerank(G, tgt))\n",
    "    print(\"Katz:\", katz(G, src, tgt))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "count = splits['test']['pos_edge_label_index'].T.size(0)\n",
    "AA = 0\n",
    "RA = 0\n",
    "Katz = 0\n",
    "for row in splits['test']['pos_edge_label_index'].T:\n",
    "    src = row[0]\n",
    "    tgt = row[1]\n",
    "    AA += adamic_adar(G, src, tgt)\n",
    "    RA += resource_allocation(G, src, tgt)\n",
    "    Katz += katz(G, src, tgt)\n",
    "    \n",
    "print(\"mAA:\", AA/count)\n",
    "print(\"mRA:\", RA/count)\n",
    "print(\"mKatz:\", Katz/count)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAG-LP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
